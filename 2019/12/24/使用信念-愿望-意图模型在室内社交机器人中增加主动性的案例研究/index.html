<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Welcome to my Hexo" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="摘要：事实证明，机器人和机器人技术的兴起在不同方面都对人类有益。机器人技术从一个简单的按钮进化而来，多年来得到了大规模的发展。因此，它已经成为人类生活的一个组成部分，因为机器人被广泛应用，从室内使用到行星际任务。">
<meta name="keywords" content="paper">
<meta property="og:type" content="article">
<meta property="og:title" content="使用信念-愿望-意图模型在室内社交机器人中增加主动性的案例研究">
<meta property="og:url" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;index.html">
<meta property="og:site_name" content="Welcome to my Hexo">
<meta property="og:description" content="摘要：事实证明，机器人和机器人技术的兴起在不同方面都对人类有益。机器人技术从一个简单的按钮进化而来，多年来得到了大规模的发展。因此，它已经成为人类生活的一个组成部分，因为机器人被广泛应用，从室内使用到行星际任务。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;2.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;3.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;4.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;5.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;6.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;7.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;8.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;9.jpg">
<meta property="og:updated_time" content="2019-12-24T15:26:59.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;isdear.github.io&#x2F;2019&#x2F;12&#x2F;24&#x2F;%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6&#x2F;1.png">

<link rel="canonical" href="http://isdear.github.io/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>使用信念-愿望-意图模型在室内社交机器人中增加主动性的案例研究 | Welcome to my Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Welcome to my Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-首页">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-档案">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>档案</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://Isdear.github.io/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/kenan.jpg">
      <meta itemprop="name" content="Kuang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to my Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          使用信念-愿望-意图模型在室内社交机器人中增加主动性的案例研究
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-12-24 23:26:59" itemprop="dateCreated datePublished" datetime="2019-12-24T23:26:59+08:00">2019-12-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><strong>摘要</strong>：事实证明，机器人和机器人技术的兴起在不同方面都对人类有益。机器人技术从一个简单的按钮进化而来，多年来得到了大规模的发展。因此，它已经成为人类生活的一个组成部分，因为机器人被广泛应用，从室内使用到行星际任务。<a id="more"></a>最近，在商业室内空间使用社交机器人提供帮助或与人进行社交互动已经相当流行。因此，考虑到越来越多的社交机器人的使用，人们已经实施了许多工作来开发机器人，使他们能够像人类一样行动。这一发展背后的理念是，需要机器人在没有被要求的情况下提供服务。社交机器人应该更像人类那样思考，通过分析他们所处的环境，提出可能的、适合的行动建议。信念-愿望-意图是开发基于人类如何基于环境中获得的信息而行动的理性代理的最流行的模型之一。因此，这项工作定义于一个基础框架，将BDI集成到一个社交机器人中以添加主动行为的“像人一样”功能。该工作通过在由机器人操作系统(ROS)操纵的室内社交机器人Waldo中使用PROFETA BDI框架开发基于视觉的主动行动，从而验证了所提议的体系结构。</p>
<p><strong>关键字</strong>：社交机器人；积极主动性；信念-愿望-意图模型(BDI)、机器人操纵系统(ROS)</p>
<h5 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h5><p>在这部分里，我们首先讨论社交机器人和主动性的概念以及在我们日常生活中的意义。然后，我们将这些概念融合在一起，从而通过使用信念-愿望-意图(BDI)模型在由机器人操作系统(ROS)驱动的室内社交机器人中增加主动性，从而形成创造附加价值的主要目标。</p>
<h6 id="1-1-社交机器人"><a href="#1-1-社交机器人" class="headerlink" title="1.1 社交机器人"></a>1.1 社交机器人</h6><p>机器人的概念给人一种类似人类的机器的印象，主要是为他们的创造者服务的。机器人为人类服务的方式有着更广阔的范围。机器人可以用于日常的家务劳动和太空探索任务，这取决于他们是如何被制造的，以及他们身上安装了多少智能设备。随着时间的推移，一定机器人的方式从“一个为工厂组装产品而建造的机械人”转变为“一个人类能与之社交互动的实体”。社交机器人的诞生受到了生物学的启发，社交机器人最初是被用于学习蜂群或昆虫的行为[1]。后来，社交机器人被用于和人类互动。</p>
<p>根据社交科学家Kate Darling称，”社交机器人是 一种身体上的体现，是一种在情感层面上与人类交流和互动的自主智能体“[2]。在这篇文章中区分社交机器人和无生命的计算机以及工业或服务机器人是很重要的（不是为了激发人类的情感和模仿社交线索设计的）。社交机器人也遵循社交行为模式，具有各种“心里状态”，并适应他们通过互动学习的内容。通常，社交机器人是以类人或类动物的形式与人类建立情感联系的，因为社交机器人的形式和形状非常重要。社会互动被期望类似于视觉和触觉感知的言语交流。根据这些交互作用，社交机器人可以分为以下几类[3]。</p>
<ol>
<li>社会唤起。这些机器人依靠人类的动作来产生一组特定的感受[4]。</li>
<li>社会地位。这些机器人对来自它们所处的社会环境的感知做出反应。机器人能够区分环境中的社会主体和对象[1]。</li>
<li>善于交际。这些机器人具有社会认知的模型，并且为了一些社会目标主动与人类接触[4]。</li>
<li>社交智慧。这些机器人尝试头基于认知和社交能力模型来复制人类的社会职能[5]。</li>
</ol>
<p>对老年人的个人护理的需求的增长和技术的进步使得社交机器人可以广泛地用于老年人护理。社交机器人形式的辅助技术帮助老年人在家中独立生活。社交机器人可以提供广泛的互动服务，如远程护理和机器人辅助治疗。这种机器人也可以用于老年人的心理和认知障碍的病人护理[6]。一些社交机器人可以监测运动、血压、呼吸或心脏问题，并在出现任何危险或风险时向相关人员发出警告[7]。社交机器人也已广泛用于与儿童的互动。一个名为Arash的社交同伴机器人被建造来为儿科医院提供治疗干预[8]。社交机器人也被用来帮助患有癌症的儿童[8]。最近有报道称，社交机器人被用于教育和照顾发育障碍儿童[9]。主动机器人已被用作家教和同伴学习者，以提供教育[10]。因此，社交机器人目前被用于医院、家庭、购物中心和会议中心，与人们互动，以欢迎、交谈或照顾人们[11]。社交机器人在与人类互动、协助、服务和探索方面都面临挑战，它们以不同的方式帮助人类。对于这些应用程序，主动行为对于社交机器人是必要的。</p>
<h6 id="1-2-主动性"><a href="#1-2-主动性" class="headerlink" title="1.2 主动性"></a>1.2 主动性</h6><p>社交机器人是自主的机器人，它按照一套为人类定义的社交规则与人类交流[12]。这些机器人使用三种不同的控制体系结构来决定对环境做出反应所需的动作——协商、反应和混合。在协商式控制中，机器人在决策时具有深思熟虑的能力，因为除了当前的传感器输入和刺激之外，机器人还具有与过去或未来状态相关的能力，可以采取相关的行动。反应控制类似于“刺激-反应”控制机制，在这种机制中，机器人通过紧密耦合感官输入和效应器输出，对不断变化的非结构化环境做出快速反应。在混合控制中，一个组件处理计划动作，而另一个组件处理通常不需要学习能力的即时反应。在混合架构中，两个不同的机制之间的耦合可能很困难，因为两个控制机制必须连续地相互通信。</p>
<p>基于这些控制架构，一个社交机器人可以通过两种方式与人类互动。机器人可以被要求为人类做事，在这种情况下，机器人是被动的。相反，机器人能够在不被要求的情况下自动帮助用户，在这些案情况下，机器人是主动的。社交机器人中的主动性概念可能是有用的实用程序，因为社交机器人主要是为了以更人性化的方式与人类互动。附录A中包含了一个示例（从[13]中采用）来理解社交机器人的反应性行为与主动性行为之间的区别。</p>
<h6 id="1-3-机器人操纵系统-ROS"><a href="#1-3-机器人操纵系统-ROS" class="headerlink" title="1.3 机器人操纵系统(ROS)"></a>1.3 机器人操纵系统(ROS)</h6><p>ROS是基于C ++的开源软件，它是用于机器人软件开发的通用软件框架，具有操作系统功能[14]。这些功能包括硬件抽象，底层设备控制，常用功能的实现，进程之间的消息传递以及程序包管理。它基于图形架构，其中每个节点从传感器，执行器接收/处理有关其状态的多个消息。 该操作系统在Linux（Ubuntu）上运行，并且可以在Windows中使用，但功能有所减少。 ROS创建了一个生态系统，其中称为节点的不同组件之间使用消息通信系统互连。 ROS是驱动商用室内机器人的关键驱动组件之一。</p>
<h6 id="1-4-信念-愿望-意图模型-BDI"><a href="#1-4-信念-愿望-意图模型-BDI" class="headerlink" title="1.4 信念-愿望-意图模型(BDI)"></a>1.4 信念-愿望-意图模型(BDI)</h6><p>BDI是通过智能编程代理构建多代理系统的主要方法之一。 该模型受人类推理的启发，并基于三个实体，即信念，愿望和意图[15]。 它提供了一种机制，用于将选择行动的活动与当前活动计划的执行分开。 系统中的代理是根据这些实体定义的。 该模型还考虑了资源限制，以便在代理进行推理后产生意图。 BDI模型希望代理在动态环境中行动，以便代理的推理应考虑环境变化以采取行动。BDI模型的三个实体解释如下：</p>
<ol>
<li>信念。 信念是代表代理人信息状态的实体。 信念反映了机器人的知识。 信念存储在信念集中。</li>
<li>愿望。 愿望是代表代理人的激励状态或代理人想要实现的目标，目的或情况的实体。 愿望是代理商想要完成的。</li>
<li>意图。 意图是代表代理的审议状态的实体。 计划是代理为实现目标而采取的一系列动作。</li>
</ol>
<p>对于社交机器人，传感器输出会构建置信集，以根据不同参数的不同值来表示机器人周围的环境。 特定的信念集描述了机器人在特定时间所在的特定情况。 根据位置，为机器人定义一个目标，该目标可以称为“期望”。 BDI解释器或引擎根据情况从计划库中选择特定的行动（意图），该计划是意图的集合。</p>
<h6 id="1-5-基本原理"><a href="#1-5-基本原理" class="headerlink" title="1.5 基本原理"></a>1.5 基本原理</h6><p>社交机器人已经成为我们生活中不可或缺的部分。从医院到像家一样的会议商店，都广泛使用社交机器人以一种或另一种方式与人类互动。现在，无论何时何地无法在特定任务中出现人员，社交机器人都可以替代人员。因此，希望机器人能够像人类一样发挥更多作用，而主动性可以成为此类机器人中的功能，它将使这种功能成为可能。许多研究试图发展机器人的主动行为。但是，当谈到由ROS提供动力的商业室内社交机器人时，还没有明确定义的系统架构和实现。考虑到此类机器人在商用室内环境中的影响，本研究试图探索增强此类商用室内社交机器人主动性的可能性。这项工作定义了一种清晰的方法，通过将基于人类推理的BDI框架集成到系统体系结构中，将社交机器人的不同活动实现为主动行为。这项工作通过在ROS操作的室内社交机器人Waldo中开发由PROFETA（BDI）框架实现的基于视觉的主动行为来验证实施。该研究的具体贡献如下：</p>
<ol>
<li>一个经过验证的模块化系统架构，具有模块化、灵活性和合理的工作分布等特点，有利于不同的逻辑块与ROS所控制的机器人集成，用于主动行为。</li>
<li>为在日常生活应用的罗斯控制机器人中开发类人行为奠定了基础。</li>
</ol>
<p>论文的其余部分组织如下:第2节讨论相关工作，第3节解释案例研究的建立以及系统概述。第4节讨论结果，第5节总结论文。</p>
<h5 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h5><p>在这一部分中，我们将描述不同BDI模型的背景，以及为集成机器人中的行为模型而构建的不同框架。此外，我们提出了不同的工作，研究人员试图将这些模型纳入机器人的各种日常生活应用。</p>
<h6 id="2-1-BDI模型"><a href="#2-1-BDI模型" class="headerlink" title="2.1 BDI模型"></a>2.1 BDI模型</h6><p>当软件代理认知模型的设计开始发挥作用时，BDI模型是最流行的体系结构选择之一。BDI模型提供了信息态度、动机态度和协商承诺的显式和说明性表示。Myers等人将BDI模型分为两大类:B-DOING和Delegative模型。在B-DOING模型中，动机态度是高度适应的，而愿望与代理人的愿望相对应。此外，义务与其他代理人的责任相对应，规范与代理人在环境中所起作用的公约相对应。为代理创建的目标需要一致且可实现的[17]。根据目标的定义，计划执行的意图。在Delegative模型中，目标被定义为候选目标和采用目标[18]。候选目标是那些内部不一致的目标，而采用的目标是BDI模型中一致和连贯的目标。该模型甚至可以将用户指定的指导和来自用户的首选项合并为建议的形式。B-DOING框架缺乏主动性帮助目标类型之间的区别，而Delegative BDI框架缺乏动机态度类型之间的区别。</p>
<h6 id="2-2-BDI框架"><a href="#2-2-BDI框架" class="headerlink" title="2.2 BDI框架"></a>2.2 BDI框架</h6><p>Russel等人开发了代理工厂框架，作为各种工具、平台和语言的开源集合，这些工具、平台和语言最终促进了多代理系统的开发和开发。Winikoff [20]构建了一个高度可移植、健壮和跨平台的环境，称为JACK，用于构建、运行和集成商用级的多代理系统。在称为JADE[21]的BDI框架中，代理平台可以分布在不同的独立机器之间，可以进行远程控制。甚至可以在运行时通过在实现期间将代理从一台机器移动到另一台机器来更改配置。Braubach 和 Pokahr [22]开发了一个基于XML和Java的JADEX框架，该框架遵循BDI模型，从工程角度简化了智能代理系统的构建。JASON是作为[24]AgentSpeak [23]的扩展而开发的超灵活平台，实现了该语言的语义，并为开发具有许多可定制特性的多代理系统提供了一个良好的平台。不同行为模型平台的比较如表1所示。</p>
<p>​                                                        表1 不同信念-愿望-意图 (BDI)平台的比较。</p>
<table>
<thead>
<tr>
<th align="center">名称</th>
<th align="center">主要领域</th>
<th align="center">资源开放</th>
<th align="center">学习能力</th>
<th align="center">程序语言</th>
</tr>
</thead>
<tbody><tr>
<td align="center">AgentFactory</td>
<td align="center">基于通用代理</td>
<td align="center">是</td>
<td align="center">平均</td>
<td align="center">Java,AgentSpeak</td>
</tr>
<tr>
<td align="center">JACK</td>
<td align="center">动态复杂环境</td>
<td align="center">否</td>
<td align="center">容易</td>
<td align="center">Java</td>
</tr>
<tr>
<td align="center">JADE</td>
<td align="center">由自治实体组成的分布式应用程序</td>
<td align="center">是</td>
<td align="center">容易</td>
<td align="center">Java</td>
</tr>
<tr>
<td align="center">JADEX</td>
<td align="center">由自治BDI实体组成的分布式应用程序</td>
<td align="center">是</td>
<td align="center">容易</td>
<td align="center">Java</td>
</tr>
<tr>
<td align="center">BDI4JADE</td>
<td align="center">企业应用程序</td>
<td align="center">是</td>
<td align="center">平均</td>
<td align="center">Java</td>
</tr>
<tr>
<td align="center">JASON</td>
<td align="center">由自治BDI实体组成的分布式应用程序</td>
<td align="center">是</td>
<td align="center">容易</td>
<td align="center">Java</td>
</tr>
<tr>
<td align="center">PROFETA</td>
<td align="center">由自主BDI实体和AI应用组成的分布式应用</td>
<td align="center">是</td>
<td align="center">容易</td>
<td align="center">Python</td>
</tr>
<tr>
<td align="center">SPYSE</td>
<td align="center">分布式人工智能应用</td>
<td align="center">是</td>
<td align="center">平均</td>
<td align="center">Python</td>
</tr>
<tr>
<td align="center">SPADE</td>
<td align="center">分布式多代理</td>
<td align="center">否</td>
<td align="center">平均</td>
<td align="center">Python</td>
</tr>
</tbody></table>
<p>ROS支持C++和Python编程语言，用于在其生态系统中的不同分布式节点之间进行通信。由于Python中有各种BDI框架，本研究考虑了pythonic框架。</p>
<h6 id="2-3-BDI模型在机器人中的应用"><a href="#2-3-BDI模型在机器人中的应用" class="headerlink" title="2.3 BDI模型在机器人中的应用"></a>2.3 BDI模型在机器人中的应用</h6><p>该行为模型适用于研究机器人与人类的自然互动，以显示出主动行为。可以想象，机器人的前瞻性行为可以增加人机交互和使用机器人的实用价值。因此，与机器人主动行为有关的工作是采用混合启动方法启动的。 Finzi和Orlandini [25]为搜索和营救行动中使用的机器人开发了一种基于计划者混合启动方法的架构。该研究具有用于执行任务的基于模型的执行监视和反应性计划程序。亚当斯等人[26]提出了一种基于效果的人机交互的混合启动交互方法。机器人会主动检测人类的情绪变化，例如检测睡意和注意力不集中。由Acosta等人开发的机器人[27]通过监视活动并将任务定义为时间表来显示一些主动行为。 Satake等人[28]提出了一种行为模型来发起与在街上行走的行人的对话。 Shi等人的工作研究了开始与人交谈或互动的适当时间。 [29]。此外，Garrel等人 [30]提出了一种主动模型的行为模型，该模型试图说服人们以不同的行为和情感发起对话。由Araiza-Illan等进行的研究[31]提出使用BDI模型来提高机器人的逼真度和类人仿真水平。实现了一个自动测试台，用于模拟人形机器人与机器人操作系统和凉亭中的人员之间的协作任务组装。 Gottifredi等人开发了一种基于BDI架构的足球比赛机器人。 [32]允许在需要时基于高层推理和反应性来规范声明性目标驱动行为。 Duffy等人的工作[33]开发了一种以自我为中心的机器人控制策略的多层BDI架构，以使机器人能够进行明确的社交行为。 Pereira等人[34]提出了BDI体系结构的扩展，以情感BDI体系结构的形式支持人工情感。</p>
<p>鉴于社交机器人主动性的现状，本研究试图扩展此类机器人的能力，将基于视觉的活动纳入社交机器人中。集成基于一个模块化架构，其他逻辑块可以很容易地集成到该架构上，以类似于人类思维的方式实现更高级的主动行为。</p>
<h5 id="3-案例研究设置"><a href="#3-案例研究设置" class="headerlink" title="3. 案例研究设置"></a>3. 案例研究设置</h5><p>在本节中，我们将解释为开发社交机器人Waldo中的主动行为而创建的用例场景。此外，我们定义了一个基于现有技术的系统框架，将OpenCV和BDI推理的模块模块与ROS生态系统集成在一起，每一步的详细说明如下。</p>
<h6 id="3-1-用例场景"><a href="#3-1-用例场景" class="headerlink" title="3.1 用例场景"></a>3.1 用例场景</h6><p>在本研究中，我们考虑在社交机器人中加入一种主动行为的基于视觉图像的活动。室内机器人Waldo的眼睛里安装了摄像头，从环境中收集图像。这些图像反馈有助于建立一种关于情况的信念。使用OpenCV中的人员检测模块处理摄像机提要。这个模块为机器人建立了一个关于环境中是否有人存在的信念。如果OpenCV模块检测到一个人，Waldo将设置一个目标，即在没有来自该人的任何显式命令的情况下问候该人。在这个实验中，机器人可以执行两个精确的动作。在检测到人之后，使用BDI框架，机器人用一句话问候被检测到的人。在一段固定的时间内，机器人不断地检测到这个人，它就会改变自己的信念，通过说出不同的句子来为这个人提供额外的帮助。对于这项工作，Waldo可以执行的操作仅限于语音，但是高级服务可以轻松地替换这些操作。这个规定是在实验中提出的，目的是为了展示信念是如何随着环境的变化而改变的，从而使机器人所采取的行动具有相关性和互动性。</p>
<h6 id="3-2-社交室内机器人Waldo"><a href="#3-2-社交室内机器人Waldo" class="headerlink" title="3.2 社交室内机器人Waldo"></a>3.2 社交室内机器人Waldo</h6><p>本研究考虑的机器人是Waldo，它是浸入式机器人[35]制造的多服务机器人。Waldo是一款具有先进视觉能力的远程呈现服务机器人。机器人有一个Arduino卡用于基本的控制功能，还有一个安装了ROS的Linux卡作为操作系统，用于更高级和复杂的功能。Waldo的可调高度为130至170厘米。这个机器人可以自主操作激光雷达、声纳、麦克风和照相机。Waldo是一个人形的室内社交机器人，用于欢迎、交谈、理解和与人交流。Waldo的移动可以通过鼠标、键盘、操纵杆、平板电脑、智能手机、平板电脑或任何其他需要的外设进行远程控制。Waldo如图1所示。<img src="/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/1.png" alt="图1 Waldo"></p>
<h6 id="3-3-系统概述"><a href="#3-3-系统概述" class="headerlink" title="3.3 系统概述"></a>3.3 系统概述</h6><p>本研究的主要目的是利用现有的技术，开发一个灵活和模块化的框架，可以方便地将不同的模块集成到框架中，<br>这最终有助于ROS控制的社交机器人主动行为的发展。在室内模型中实现环境行为模型的整体系统概况如图2所示。安装在室内机器人Waldo上的不同传感器收集环境信息。摄像头、激光雷达和Kinect传感器帮助收集特定时刻的环境信息。信息被中继到Waldo PC，它通过无线连接连接到监控PC。由于Waldo PC的功能有限，计算密集型逻辑模块可以在功能更强大的监视PC上运行。逻辑模型负责从机器人传感器收集的环境数据中获取各种知识。基于这个知识库，BDI框架为任何给定的时刻建立信念和目标。框架还从预定义的计划列表中选择一组操作来实现目标。这些动作通过无线连接转发到Waldo PC，并指导Waldo中的不同执行器执行广泛的动作。ROS中的内部机制管理节点与Waldo PC之间的通信。安装了Linux和ROS的Waldo PC可以安装几个逻辑模块，或者监视PC以建立一个或多个关于环境的信念。逻辑层中的模块可以是BDI框架、用于视觉处理的OpenCV，以及用于建立关于环境的重要信念的其他智能模块。BDI框架响应已建立的信念，并为任何时刻设置目标。框架中的引擎从预定义的库中选择一个行动计划来实现目标。计划执行转发给Waldo PC，它在机器人中产生实际的动作来响应环境。可以使用通过无线网络连接到Waldo PC的监视PC来监视流操作。<img src="/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/2.jpg" alt="图2 系统概述"></p>
<h6 id="3-4-BDI建模"><a href="#3-4-BDI建模" class="headerlink" title="3.4 BDI建模"></a>3.4 BDI建模</h6><p>问题的BDI建模应该能够有效地回答以下问题。</p>
<ul>
<li>当采取行动?只要机器人发现房间里有人，它就会工作。因此，一个有效的人的检测机制应该集成到机器人。必须采用有效的块来触发动作。</li>
<li>采取什么行动？机器人的动作很大程度上取决于对人的检测。这个动作可以是一个简单的问候语传递给某人，也可以是传递给提供帮助或根本不采取任何行动的人。</li>
<li>如何执行这些操作?基于对环境的一系列信念，机器人可以决定采取行动。在任何检测到人的情况下，机器人会利用它的文本到语音节点说出句子来问候或帮助人们。对于不被发现的人，机器人可以故意保持空闲或进入睡眠模式。</li>
</ul>
<p>机器人眼睛上安装的摄像头收集周围的知识。在实验中，只使用了摄像机的输入。来自OpenCV的人员检测块用于显式地定义系统设计的信念。此外，目标和行动被相应地定义，以实现主动行为的用例。机器人可能采取的行动仅限于语言。根据案例的BDI模型，一组信念、愿望和意图研究定义如下:</p>
<p><strong>Belief</strong>: personDetected(“Yes”), personDetected(“No”) and personDetected(“Next”)</p>
<p><strong>Desire</strong>: DoNothing(), GreetPeople() and OfferHelp()</p>
<p><strong>Intentions</strong>: stayIdle(), speak()</p>
<hr>
<p>PROFETA框架可以通过以下步骤来实现行为建模:</p>
<p>算法1 PROFETA框架实现。</p>
<p>1:导入必要的PROFETA库</p>
<p>2:在脚本中将信念和目标定义为类</p>
<p>3:通过创建类并覆盖execute()方法来定义用户操作</p>
<p>4:启动PROFETA引擎</p>
<p>5:使用声明性语法定义规则</p>
<p>6 .发动引擎</p>
<hr>
<p>此外，PROFETA框架还促进了传感器类的定义，传感器类本身可以根据环境添加或删除一组信念。这可以在PROFETA中通过声明一个子类Sensor、重写sense（）方法并通知PROFETA引擎程序中添加了一个新的传感器来完成。PROFETA使用陈述性语言来表达代理人的行为。代理行为的声明性语法描述如下：“Event/”Condition“ &gt;&gt; ”setofActions”</p>
<p>在这种声明性语法中，事件可以是信念断言或撤回、目标完成或请求，甚至是目标失败中的任何一个。语法中的条件是指一组特定的知识库，而操作可以是目标完成请求、用户定义的操作集或添加或删除信仰。这种语法可以举例说明：</p>
<p>+objectAt(“A”, “B”)/objectGot(“no”) » [moveTo(“A”,“B”), pickObject()]</p>
<h6 id="3-5-使用OpenCV进行人身检测"><a href="#3-5-使用OpenCV进行人身检测" class="headerlink" title="3.5 使用OpenCV进行人身检测"></a>3.5 使用OpenCV进行人身检测</h6><p>对于人的检测，开源计算机视觉库（OpenCV）是一个免费提供的用于计算机视觉和机器学习的开源库。OpenCV中的库和算法直接用于基于HOG特征直方图和支持向量机分类器的人体检测实验。OpenCV算法的性能改进超出了本文的研究范围。ROS有自己的图像格式，用于通过订阅和发布在节点之间进行通信。此图像格式必须转换为OpenCV格式才能使用OpenCV库进行人员检测。CvBridge是ROS中的一个库，它促进了ROS图像到OpenCV图像格式的转换，反之亦然。CvBridge接口如图3所示。<img src="/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/3.jpg" alt="图3 CvBridge 接口"></p>
<h6 id="3-6-实验装置"><a href="#3-6-实验装置" class="headerlink" title="3.6 实验装置"></a>3.6 实验装置</h6><p>测试用例在Waldo中实现，工作分布在两台PC上。工作站PC在Intel i5-5300U处理器上安装了Ubuntu 16.04，而Waldo PC在Intel(R) Atom(TM)处理器上安装了相同的操作系统。Waldo的主动行为在PROFETA的BDI框架中得到了很好的实现。</p>
<h5 id="4-结果与讨论"><a href="#4-结果与讨论" class="headerlink" title="4 结果与讨论"></a>4 结果与讨论</h5><p>在本节中，我们将验证所提出的系统设计，以将各种逻辑模块(如BDI模型和OpenCV)集成到ROS中，实现主动行为。使用定性方法验证了所提出框架的功能，并研究了模块化、灵活性和合理的工作分配等几个特性。此外,我们详细解释了相关的发现，并对逻辑OpenCV模块的结果进行了定量分析。</p>
<h6 id="4-1-系统架构中的工作分配"><a href="#4-1-系统架构中的工作分配" class="headerlink" title="4.1 系统架构中的工作分配"></a>4.1 系统架构中的工作分配</h6><p>该框架的一个重要特点是合理分配机器人主动行为开发所需的工作。本研究使用PROFETA的BDI框架的一个覆盖层来实现ROS中的测试用例。OpenCV库中的一个模块进行图像处理。该系统设计将操作分配到两台PC上，即安装在机器人和工作站PC上的Waldo PC。测试实验使用了ROS的分布式工作架构，将Waldo PC从繁重的图像处理中解脱出来。人员检测和实用推理模块(BDI框架)安装在一个功能相对强大的工作站PC上。在Waldo PC中处理实际操作和不同ROS节点的管理。这些操作不是计算密集型的。因此，所提出的体系结构在开发主动性行为时支持合理的工作分配。因此，更高级的活动可以被认为是一个扩展，因为更多的计算密集型模块可以很容易地集成到机器人中，这要感谢它的架构。更强大的机器可以承担工作站PC的角色，而Waldo PC可以承担信息收集和效应器的轻角色。此外，该框架允许我们添加额外的计算设备，以考虑各种主动行为所需的不同工作负载。这些功能也使所提议的框架具有灵活性。</p>
<h6 id="4-2-用测试用例验证提出的系统设计"><a href="#4-2-用测试用例验证提出的系统设计" class="headerlink" title="4.2 用测试用例验证提出的系统设计"></a>4.2 用测试用例验证提出的系统设计</h6><p>该系统设计的验证主要研究了在Waldo的主动性行为展示过程中是否实现了灵活性、模块化和合理的工作分配。为了验证所提出的系统设计，我们在ROS生态系统内外创建了几个分布式节点。每个ROS节点分别用于摄像机馈送、人员检测块和语音块。不同的ROS节点通过主题消息相互通信。Roscore管理节点之间的通信。根据需要，可以创建和删除节点来添加或删除功能。可以在提议的框架内的任何计算设备中创建节点，从而提供灵活性和模块化。最初，摄像机节点发布Waldo眼睛收集的图像提要。转换器节点通过CvBridge接口订阅摄像机节点的主题消息，并将ROS图像转换为OpenCV图像。然后节点发布转换后的图像。在工作站PC中有一个名为person detector的节点，它订阅转换后的图像消息。该节点执行OpenCV库的person检测模块。在工作站PC中有一个名为BDI engine的附加节点，它订阅由person检测器发布的关于人员检测的消息。BDI引擎执行所有必要的行为建模，以发布最终由机器人完成的动作。还有另一个节点listener订阅BDI引擎节点并发布机器人中执行器的消息以执行操作。订阅侦听器的语音节点使机器人说出句子以达到目的。在广泛分布的ROS生态系统中，不同节点之间的整个交互作用如图4所示。从图中可以看出，主动行为所需的不同工作负载分布在Waldo PC和Workstation PC上。这种合理的工作分配是建议的框架的优点之一，可以根据需求轻松地添加或删除计算设备。此外，在建议的框架中，可以创建额外的逻辑块/模块作为ROS生态系统中的新节点，以开发社交机器人的额外功能。在我们的研究中，我们创建了一个节点，用于在人员检测期间机器人头部的移动，以演示所提出的框架中的模块化。新创建的节点与person检测器节点通信以创建任何移动。<img src="/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/4.jpg" alt="图4 机器人操作系统(ROS)生态系统中不同节点之间的交互作用"></p>
<p>4.3 性能分析</p>
<p>为了进行性能分析和评估，Waldo保持在走廊的固定位置。 装有相机的机器人眼睛的高度约为162厘米。 重复进行测试，其中自然光和人造光均会影响照明条件。</p>
<p>图5和图6分别表示未检测到人员和检测到人员的情况下BDI动作和由安装在机器人中的摄像机收集的图像馈送的执行情况。当在图像供稿中未检测到人时，BDI引擎会建立对环境的信念，并建立不向人打招呼的目标。因此，引擎选择NoTalk的动作来实现该目标，如图5a所示。同样，当检测到人员时，BDI引擎会通过在工作站PC上运行的人员检测模块来启用遇到人员的信念。基于这种信念，引擎必须设定一个目标，要么向人们打招呼，要么向人们提供额外的帮助。为了两者之间的区别，我们添加了跟踪遇到该人多长时间的逻辑操作，如图6a中的计数器所示。基于在图像馈送中检测到的计数器和人员的值，BDI引擎建立了两个不同的信念集，需要两个不同的目标。实现问候对象目标的问候动作在图6a中表示为交谈动作。执行帮助动作以实现在更长的时间内连续检测到人时向他人提供帮助的目的。<img src="/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/5.jpg" alt="图5 案例:未检测到人员(a) BDI执行情况(b) Waldo眼睛采集的图像"><img src="/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/6.jpg" alt="图6 案例：检测到人员（a）执行BDI和（b）Waldo的眼睛收集到的图像供稿。此外，我们在两个不同的场景中考虑了距机器人的不同距离，测试了整个系统的工作情况。 表2和表3给出了性能分析。"></p>
<p>​                                                            表2.验证工作的距离考虑</p>
<table>
<thead>
<tr>
<th align="center">距离</th>
<th align="center"></th>
<th align="center">场景1</th>
<th align="center"></th>
<th align="center"></th>
<th align="center">场景2</th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="center">人与相机</td>
<td align="center">准确</td>
<td align="center">误报</td>
<td align="center">漏报</td>
<td align="center">准确</td>
<td align="center">误报</td>
<td align="center">漏报</td>
</tr>
<tr>
<td align="center">&lt;2 m</td>
<td align="center">253</td>
<td align="center">160</td>
<td align="center">187</td>
<td align="center">175</td>
<td align="center">160</td>
<td align="center">265</td>
</tr>
<tr>
<td align="center">2–10 m</td>
<td align="center">401</td>
<td align="center">87</td>
<td align="center">112</td>
<td align="center">354</td>
<td align="center">114</td>
<td align="center">132</td>
</tr>
</tbody></table>
<p>​                                                            表3.准确性和召回率评估</p>
<table>
<thead>
<tr>
<th align="center">距离</th>
<th align="center"></th>
<th align="center">场景1</th>
<th align="center"></th>
<th align="center"></th>
<th align="center">场景2</th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="center">人与相机</td>
<td align="center">精度</td>
<td align="center">召回</td>
<td align="center">F1 分数</td>
<td align="center">准确</td>
<td align="center">召回</td>
<td align="center">F1 分数</td>
</tr>
<tr>
<td align="center">&lt;2 m</td>
<td align="center">0.612</td>
<td align="center">0.575</td>
<td align="center">0.593</td>
<td align="center">0.52</td>
<td align="center">0.398</td>
<td align="center">0.451</td>
</tr>
<tr>
<td align="center">2–10 m</td>
<td align="center">0.822</td>
<td align="center">0.782</td>
<td align="center">0.801</td>
<td align="center">0.756</td>
<td align="center">0.728</td>
<td align="center">0.742</td>
</tr>
</tbody></table>
<p>整个系统的性能分析表明，机器人在2-10 m以上的区域表现较差。场景1的准确率和召回率分别为0.822和0.782，场景2的准确率和召回率分别为0.756和0.728。与场景1相比，场景2有更多的光照不一致。测试期间的漏报和准确（如图7和8所示）是由于物体移动引起的照明条件变化引起的。此外，这些负面因素是由几种光源（自然和人工）形成的人的阴影造成的。由于数据集分布不均，我们还针对每种情况计算了F1-Score。对于场景1，在2-10 m的区域中，最佳F1分数为0.801。 F1分数的高值（接近1）表明模块在检测到人时的效率，从而使机器人可以表现出主动的招呼行为并为被检测到的人提供帮助。与精度和召回率的分析相似，场景2的F1得分在&lt;2 m范围内最低，这突出说明了逻辑块在该位置执行的效率不高。此外，我们绘制了实验的精确召回曲线，如图9所示。该曲线还证实了这一发现，并确定了场景1（2-10 m区域）中模块的最佳性能，作为该曲线下的面积。情况是最高的（请参见图9）。模块的性能受多种因素影响，例如人造光，阴影，多种光源以及相机与人之间的距离。通过增强OpenCV逻辑块的性能，可以提高给定测试用例中系统设计的准确性。<img src="/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/7.jpg" alt="图7 漏报"><img src="/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/8.jpg" alt="图8 误报"><img src="/2019/12/24/%E4%BD%BF%E7%94%A8%E4%BF%A1%E5%BF%B5-%E6%84%BF%E6%9C%9B-%E6%84%8F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%AE%A4%E5%86%85%E7%A4%BE%E4%BA%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%B8%BB%E5%8A%A8%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6/9.jpg" alt="图9 精度-召回 曲线"></p>
<h6 id="4-4-限制"><a href="#4-4-限制" class="headerlink" title="4.4 限制"></a>4.4 限制</h6><p>在这项研究中，不考虑没有人的全身的图像帧。由于摄像机放置在一定的高度上，头部的角度运动是有限的，所以摄像机无法覆盖小于一米的距离。因此，机器人前方不到2米的区域是一个盲点。这种盲点在人的检测方面表现不佳。此外，在约10 m的距离之外，人检测模块无法检测到人。至于系统设计，该架构仍然提供模块化和灵活性。整个系统的准确性在很大程度上取决于用于开发机器人主动行为的逻辑块。除了逻辑块的局限性之外，本研究更多地关注了所提出的框架的定性验证，其中研究了模块性、灵活性和合理的工作分布等特征。这项工作主要是在开发一个基本灵活的基础上，利用现有的和自由可用的工具来开发社会机器人中的主动行为。由于没有现成的可比较的体系结构和案例研究，本文不包括比较分析。此外，社交机器人的先进能力目前也未被考虑。这些研究的局限性将在未来得到加强。</p>
<h5 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h5><p>最近，在商业空间（包括家庭）中，类人和类动物室内社交机器人的兴起，使得这些机器人的“类人行为”行为成为一种更友好的人机交互的必要条件。这种社交机器人的主动性为机器人增加了更多的效用。因此，在本文中，我们提出了一个在室内商业社交机器人中这种主动行为的验证用例，Waldo由行为模型框架PROFETA支持。我们明确地定义了具有灵活性、模块性和合理工作分布的基本系统体系结构，以将BDI框架集成到ROS的分布式生态系统中。在该体系结构中，多个ROS节点可以通过无线通信在多台机器上独立创建。我们演示了如何使用外部模块（如OpenCV库）以即插即用的方式增强室内机器人的能力。我们希望所提出的系统架构奠定了坚实的基础，以开发广泛的主动行为在室内社会机器人的行为和行为像一个人。通过在所提出的体系结构中添加各种逻辑模块，可以实现这种行为。</p>
<p>通过机器人的基本动作验证了所提出的结构的有效性。目前正在进行初步工作，将人工智能的各个模块集成到所提出的体系结构中，以便在机器人中开发更多的智能动作。Waldo可以执行的操作的扩展也在进行中。未来的工作可以集中在通过使用机器人中多个传感器收集的数据，在人类推理范式BDI中建立更精确的信念。除了相机，激光雷达和Kinect传感器可以更好地代表环境的状态。在机器人的行为方面，进一步的工作可以集中在融合机器人的自主导航来实现基于不同信念的目标设置上。学习机制的加入，不断完善，可以看作是一个必不可少的延伸。</p>
<p><strong>作者贡献</strong>：U.K.C.促成了项目概念化、文献综述、方法论、工具集成、测试、验证和实验。J.C. 协助项目开发、管理、概念化、方法、验证和监督。两位作者都对论文的撰写、评论和编辑做出了贡献。</p>
<p><strong>资金</strong>：这个调查没有收到外部的资金。</p>
<p><strong>致谢</strong>：作者感谢Orange Labs, Lannion的Home and Family Communication (HFC)团队所有成员在项目期间给予的帮助和支持。同时，作者也非常感谢在论文发表的各个阶段帮助我们提高论文质量的每一个人。</p>
<p><strong>利益冲突</strong>:作者声明没有利益冲突。</p>
<p>附录 A</p>
<p>​                                                                        表A1 反应性和主动性行为</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">反应性</th>
<th align="center">主动性</th>
</tr>
</thead>
<tbody><tr>
<td align="center">User</td>
<td align="center">Could you  help me?</td>
<td align="center"><strong>It seems like your computer</strong></td>
</tr>
<tr>
<td align="center">Robot</td>
<td align="center">Yes, What  is wrong?</td>
<td align="center"><strong>is  not responding, let me know if I can do anything?</strong></td>
</tr>
<tr>
<td align="center">User</td>
<td align="center">It seems like my computer is not  responding</td>
<td align="center">Could you  help me?</td>
</tr>
<tr>
<td align="center">Robot</td>
<td align="center">You should  end the process</td>
<td align="center">You should  end the process</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">by  pressing ALT+CTRL+DEL.</td>
<td align="center">by  pressing ALT+CTRL+DEL.</td>
</tr>
<tr>
<td align="center">User</td>
<td align="center">Won’t that  restart my computer?</td>
<td align="center">Won’t that  restart my computer?</td>
</tr>
<tr>
<td align="center">Robot</td>
<td align="center">Don’t  worry; it will just</td>
<td align="center">Don’t  worry; it will just</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">end the  process.</td>
<td align="center">end the  process.</td>
</tr>
<tr>
<td align="center">User</td>
<td align="center">Hmm, it’s  not doing anything.</td>
<td align="center">OK</td>
</tr>
<tr>
<td align="center">Robot</td>
<td align="center">Press  ALT+CTRL+DEL together.</td>
<td align="center">Press  ALT+CTRL+DEL together.</td>
</tr>
<tr>
<td align="center">User</td>
<td align="center">Ok and  now?</td>
<td align="center">Then you  should end the process.</td>
</tr>
<tr>
<td align="center">Robot</td>
<td align="center">Then you  should end the process.</td>
<td align="center"><strong>Do you have a back-up?</strong></td>
</tr>
<tr>
<td align="center">User</td>
<td align="center">But then I  lost my work?</td>
<td align="center">I’m not  sure.</td>
</tr>
<tr>
<td align="center">Robot</td>
<td align="center">Don’t  worry your computer makes a</td>
<td align="center">Don’t  worry your computer makes a</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">backup  every five minutes</td>
<td align="center">backup  every five minutes</td>
</tr>
<tr>
<td align="center">User</td>
<td align="center">How can I  find a backup?</td>
<td align="center">Where is  my backup?</td>
</tr>
<tr>
<td align="center">Robot</td>
<td align="center">You can  open your program</td>
<td align="center">To find  your program, you can</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">again and  select the file.</td>
<td align="center">open your program again and  select the file. Try it!</td>
</tr>
</tbody></table>
<h6 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h6><ol>
<li><p>Fong, T.; Nourbakhsh, I.; Dautenhahn, K. A survey of socially interactive robots. <em>Robot. Auton. Syst.</em> <strong>2003</strong>, <em>42</em>, 143–166.</p>
</li>
<li><p>Campa, R. The rise of social robots: a review of the recent literature. <em>J. Evol. Technol.</em> <strong>2016</strong>, <em>26</em>, 106–113.</p>
</li>
<li><p>Dautenhahn, K. Socially intelligent robots: dimensions of human–robot interaction. <em>Philos. Trans. R. Soc. B Biol. Sci.</em> <strong>2007</strong>, <em>362</em>, 679–704.</p>
</li>
<li><p>Breazeal, C.L. <em>Designing Sociable Robots</em>; MIT Press: Cambridge, MA, USA, 2004.</p>
</li>
<li><p>Dautenhahn, K. The art of designing socially intelligent agents: Science, fiction, and the human in the loop.</p>
</li>
</ol>
<p><em>Appl. Artif. Intell.</em> <strong>1998</strong>, <em>12</em>, 573–617.</p>
<ol start="6">
<li>Ferland, F.; Agrigoroaie, R.; Tapus, A. Assistive Humanoid Robots for the Elderly with Mild Cognitive</li>
</ol>
<p>Impairment. In <em>Humanoid Robotics: A Reference</em>; Springer: Berlin/Heidelberg, Germany, 2019; pp. 2377–2396.</p>
<ol start="7">
<li>Flandorfer, P. Population ageing and socially assistive robots for elderly persons: The importance of sociodemographic factors for user acceptance. <em>Int. J. Popul. Res.</em> <strong>2012</strong>, <em>2012</em>, 829835.</li>
<li>Meghdari, A.; Shariati, A.; Alemi, M.; Vossoughi, G.R.; Eydi, A.; Ahmadi, E.; Mozafari, B.; Amoozandeh Nobaveh, A.; Tahami, R. Arash: A social robot buddy to support children with cancer in a hospital environment. <em>Proc. Inst. Mech. Eng. Part H J. Eng. Med.</em> <strong>2018</strong>, <em>232</em>, 605–618.</li>
<li>Conti, D.; Di Nuovo, S.; Buono, S.; Di Nuovo, A. Robots in education and care of children with developmental disabilities: a study on acceptance by experienced and future professionals. <em>Int. J. Soc. Robot.</em> <strong>2017</strong>, <em>9</em>, 51–62. </li>
<li>Belpaeme, T.; Kennedy, J.; Ramachandran, A.; Scassellati, B.; Tanaka, F. Social robots for education: A review.</li>
</ol>
<p><em>Sci. Robot.</em> <strong>2018</strong>, <em>3</em>, eaat5954.</p>
<ol start="11">
<li><p>Siciliano, B.; Khatib, O. <em>Springer Handbook of Robotics</em>; Springer: Berlin/Heidelberg, Germany, 2016.</p>
</li>
<li><p>Bartneck, C.; Forlizzi, J. A design-centred framework for social human-robot interaction. In Proceedings of the RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No. 04TH8759), Kurashiki, Japan, 22–22 September 2004; pp. 591–594.</p>
</li>
<li><p>Kemper, N. Effects of Proactive Behavior and Physical Interaction with a Social Robot. Master’s Thesis, University of Amsterdam, Amsterdam, The Netherlands, 2009.</p>
</li>
<li><p>Quigley, M.; Conley, K.; Gerkey, B.; Faust, J.; Foote, T.; Leibs, J.; Wheeler, R.; Ng, A.Y. ROS: An open-source Robot Operating System. In Proceedings of the ICRA Workshop on Open Source Software, Kobe, Japan, 17 May 2009; Volume 3, p. 5.</p>
</li>
<li><p>Bratman, M. <em>Intention, Plans, and Practical Reason</em>; Harvard University Press Cambridge: Cambridge, MA, USA, 1987; Volume 10.</p>
</li>
<li><p>Myers, K.; Yorke-Smith, N. Proactive behavior of a personal assistive agent. In Proceedings of the AAMAS Workshop on Metareasoning in Agent-Based Systems, Honolulu, HI, USA, 14 May 2007; pp. 31–45.</p>
</li>
<li><p>Dignum, F.; Kinny, D.; Sonenberg, L. From desires, obligations and norms to goals. <em>Cogn. Sci. Q.</em> <strong>2002</strong>, <em>2</em>, 407–430.</p>
</li>
<li><p>Myers, K.L.; Yorke-Smith, N. A cognitive framework for delegation to an assistive user agent. In Proceedings of the AAAI 2005 Fall Symposium on Mixed-Initiative Problem-Solving Assistants, Arlington, Virginia, 4–6 November 2005; pp. 94–99.</p>
</li>
<li><p>Russell, S.; Jordan, H.; O’Hare, G.M.; Collier, R.W. Agent factory: A framework for prototyping logic-based AOP languages. In Proceedings of the German Conference on Multiagent System Technologies, Berlin, Germany, 6–7 October 2011; pp. 125–136.</p>
</li>
<li><p>Winikoff, M. JACKTM intelligent agents: An industrial strength platform. In <em>Multi-Agent Programming</em>; Springer: Berlin/Heidelberg, Germany, 2005; pp. 175–193.</p>
</li>
<li><p>Jedrzejowicz, P.; Wierzbowska, I. JADE-Based a-team environment. In Proceedings of the International Conference on Computational Science, Reading, UK, 28–31 May 2006; pp. 719–726.</p>
</li>
<li><p>Braubach, L.; Pokahr, A. The jadex project: Simulation. In <em>Multiagent Systems and Applications</em>; Springer: Berlin/Heidelberg, Germany, 2013; pp. 107–128.</p>
</li>
<li><p>Rao, A.S. AgentSpeak (L): BDI agents speak out in a logical computable language. In Proceedings of the European Workshop on Modelling Autonomous Agents in a Multi-Agent World, Eindhoven, The Netherlands, 22–25 January 1996; pp. 42–55.</p>
</li>
<li><p>Bordini, R.H.; Hübner, J.F.; Wooldridge, M. <em>Programming Multi-Agent Systems in AgentSpeak Using Jason</em>; John Wiley &amp; Sons: Hoboken, NJ, USA, 2007; Volume 8.</p>
</li>
<li><p>25.Finzi, A.; Orlandini, A. Human-Robot Interaction Through Mixed-Initiative Planning for Rescue and Search Rovers. In <em>AI*IA 2005: Advances in Artificial Intelligence</em>; Bandini, S., Manzoni, S., Eds.; Springer: Berlin/Heidelberg, Germany, 2005; pp. 483–494.</p>
</li>
<li><p>Adams, J.A.; Rani, P.; Sarkar, N. Mixed initiative interaction and robotic systems. In Proceedings of the AAAI Workshop on Supervisory Control of Learning and Adaptive Systems, San Jose, CA, USA, 25–26 July 2004; pp. 6–13.</p>
</li>
<li><p>27.Acosta, M.; Kang, D.; Choi, H.J. Robot with emotion for triggering mixed-initiative interaction planning. In Proceedings of the 2008 IEEE 8th International Conference on Computer and Information Technology Workshops, Sydney, Australia, 8–11 July 2008; pp. 98–103.</p>
</li>
<li><p>28.Satake, S.; Kanda, T.; Glas, D.F.; Imai, M.; Ishiguro, H.; Hagita, N. How to approach humans?: Strategies for social robots to initiate interaction. In Proceedings of the 4th ACM/IEEE International Conference on Human Robot Interaction, La Jolla, CA, USA, 9–13 March 2009; pp. 109–116.</p>
</li>
<li><p>Shi, C.; Shimada, M.; Kanda, T.; Ishiguro, H.; Hagita, N. Spatial Formation Model for Initiating Conversation. In Proceedings of the 7th Annual Robotics: Science and Systems Conference, Los Angeles, CA, USA, 16–20 October 2011; doi:10.15607/RSS.2011.VII.039.</p>
</li>
<li><p>Garrell, A.; Villamizar, M.; Moreno-Noguer, F.; Sanfeliu, A. Proactive behavior of an autonomous mobile robot for human-assisted learning. In Proceedings of the 2013 IEEE RO-MAN, Gyeongju, Korea, 26–29 August 2013; pp. 107–113.</p>
</li>
<li><p>Araiza-Illan, D.; Pipe, T.; Eder, K. Model-Based Testing, Using Belief-Desire-Intentions Agents, of Control Code for Robots in Collaborative Human-Robot Interactions. <em>arXiv</em> <strong>2016</strong>, arXiv:1603.00656.</p>
</li>
<li><p>Gottifredi, S.; Tucat, M.; Corbatta, D.; García, A.J.; Simari, G.R. A BDI architecture for high level robot deliberation. In Proceedings of the XIV Congreso Argentino de Ciencias de la Computación, Chilecito, Argentina, 8–12 August 2008.</p>
</li>
<li><p>Duffy, B.R.; Dragone, M.; O’Hare, G.M. Social robot architecture: A framework for explicit social interaction. In Proceedings of the Toward Social Mechanisms of Android Science: A CogSci 2005 Workshop, Stresa, Italy, 25–26 July 2005.</p>
</li>
<li><p>Pereira, D.; Oliveira, E.; Moreira, N.; Sarmento, L. Towards an architecture for emotional BDI agents. In Proceedings of the 2005 Portuguese Conference on Artificial Intelligence, Covilha, Portugal, 5–8 December 2005; pp. 40–46.</p>
</li>
<li><p>Immersive Robotics. Available online: <a href="http://immersive-robotics.com/" target="_blank" rel="noopener">http://immersive-robotics.com/</a> (accessed on 12 February 2017).</p>
</li>
</ol>
<hr>
<p>Received: 12 September 2019; Accepted: 19 November 2019; Published: 20 November 2019</p>
<p><strong>Abstract:</strong> The rise of robots and robotics has proved to be a benefaction to humankind in different aspects. Robotics evolved from a simple button, has seen massive development over the years. Consequently, it has become an integral part of human life as robots are used for a wide range of applications ranging from indoor uses to interplanetary missions. Recently, the use of social robots, in commercial indoor spaces to offer help or social interaction with people, has been quite popular. As such, taking the increasing use of social robots into consideration, many works have been carried out to develop the robots to make them capable of acting like humans. The notion behind this development is the need for robots to offer services without being asked. Social robots should think more like humans and suggest possible and suitable actions by analyzing the environment where they are. Belief–desire–intention (BDI) is one of the most popular models for developing rational agents based on how humans act based on the information derived from an environment. As such, this work defines a foundation architecture to integrate a BDI framework into a social robot to add</p>
<p>“act like a human” feature for proactive behaviors. The work validates the proposed architecture by developing a vision-based proactive action using the PROFETA BDI framework in an indoor social robot, <em>Waldo</em>, operated by the robot operating system (ROS).</p>
<p><strong>Keywords:</strong> social robots; proactivity; belief–desire–intention (BDI) model; robot operating system</p>
<p>(ROS)</p>
<p>According to social scientist Kate Darling, <em>“A social robot is a physically embodied, autonomous agent that communicates and interacts with humans on an emotional level”</em> [2]. For this article, it is important to distinguish social robots from inanimate computers and industrial or service robots (not designed to elicit human feelings and mimic social cues). Social robots also follow social behavior patterns, have various “states of mind”, and adapt to what they learn through their interactions. Usually, the social robots are in humanoid or animaloid form to create an emotional connection with the human as forms and shapes of social robots are very important. The social interaction is expected to be similar to verbal communication with visual and tactile perception. Based on the interactions, social robots can be classified into the following categories [3].</p>
<ol>
<li><p>Socially evocative. These robots rely on human action to generate a particular set of feelings [4].</p>
</li>
<li><p>Socially situated. These robots react to the perceptions derived from the social environment in which they are situated. The robots can distinguish the social agents and objects in the environment [1].</p>
</li>
<li><p>Sociable. These robots have models for social cognition and proactively engage with the human for some social aims [4].</p>
</li>
<li><p>Socially intelligent. These robots try to replicate the social intelligence of humans based on the model of cognition and social competence [5].</p>
</li>
</ol>
<p>The rise of demand for personal care for ageing people and technological advancements have made it possible for social robots to be used extensively for elderly care. Assistive technologies in the form of social robots help the older population to live an independent life in their homes. Social robots can provide a wide range of interactive services like telecare and robot-assisted therapy. Such robots can be used for patient care for older people with mental and cognitive impairments as well [6]. Some of the social robots can monitor the movement, blood pressure, breathing or heart problems and warn a related person in case of any danger or risk [7]. Social robots have been used for a wide range of interactions with children as well. A social companion robot called ’Arash’ was built to provide therapeutic intervention in pediatric hospitals [8]. The social robot was also utilized to assist the children with cancer [8]. Social robots have been recently reported to be used in the education and care of children with developmental disabilities [9]. Proactive robots have been used as tutors and peer learners to deliver education as presented in [10]. As such, social robots are currently used in hospitals, homes, shopping malls, and convention centers to interact with people to either welcome, converse or take care of the people [11]. Social robots have taken challenges of interacting with, assisting, serving and exploring with humans to help humans in different ways. For these applications, proactive behaviors are necessary for social robots.</p>
<h2 id="1-2-Proactivity"><a href="#1-2-Proactivity" class="headerlink" title="1.2. Proactivity"></a><em>1.2. Proactivity</em></h2><p>Social robots are autonomous robots that communicate with humans following a set of social rules defined for them [12]. These robots use three different control architectures to decide the actions required to respond to the environment—deliberative, reactive and hybrid. In deliberative control, robots have thoughtfulness in decision making as there are capacities to relate to the past or future states, beyond the present sensor inputs and stimuli, to take relevant action. Reactive control is similar to ‘stimulus-response’ control mechanism in which the robots respond very quickly to changing and unstructured environments by tightly coupling the sensory inputs and effector outputs. In the hybrid control, one component deals with planning the actions while other deals with immediate reactions that do not usually require learning abilities. The coupling of two different mechanisms in hybrid architecture can be difficult as two control mechanisms have to communicate with each other continuously.</p>
<p>Based on these control architectures, a social robot can interact with humans in two ways. The robots can be asked to do things for humans and in such cases, the robots are reactive. In contrast, robots can automatically help the user without being asked, and in such cases, robots are proactive.</p>
<p>The notion of proactivity in social robots can be a useful utility as social robots are primarily meant to interact with the human in a more humanly ways. An example (adopted from [13]) to understand the difference between reactive and proactive behavior in social robots is included in Appendix A.</p>
<h2 id="1-3-Robot-Operating-System-ROS"><a href="#1-3-Robot-Operating-System-ROS" class="headerlink" title="1.3. Robot Operating System (ROS)"></a><em>1.3. Robot Operating System (ROS)</em></h2><p>ROS is an open-source, C++ based, a general software framework for robot software development which gives operating system functionalities [14]. Those functionalities are hardware abstraction, low-level device control, implementation of commonly-used features, message-passing between processes, and package management. It is based on a graph architecture where each node receives and process several messages from/to sensor, actuators about their state. This operating system works on top of Linux (Ubuntu) and can be used in Windows with some reduced features. ROS creates an ecosystem where different components called nodes are interconnected using a system of message communication between them. ROS is one of the key driving components, that power the commercial indoor robots.</p>
<h2 id="1-4-BDI"><a href="#1-4-BDI" class="headerlink" title="1.4. BDI"></a><em>1.4. BDI</em></h2><p>BDI is one of the major approaches to build a multi-agent system by intelligent programming agents. This model is inspired by human reasoning and based on three entities, namely, beliefs, desire, and intentions [15]. It gives a mechanism for separating the activity of choosing an action from the execution of currently active plans. The agents in a system are defined in terms of these entities. This model also considers the resource bound so that an intention will result after the agent’s reasoning. BDI model expects the agent to act in a dynamic environment such that the agent’s reasoning should take the environment changes into account to make an action. The three entities of a BDI model are explained as follow:</p>
<ol>
<li><p>Beliefs. Beliefs are the entities that represent the informational state of the agent. Beliefs reflect the knowledge of the robot. Beliefs are stored in belief sets.</p>
</li>
<li><p>Desires. Desires are the entities that represent the motivational state of an agent or the goals, objectives or situation which the agent would like to achieve. Desires are what the agent wants to accomplish.</p>
</li>
<li><p>Intentions. Intentions are the entities that represent the deliberative state of an agent. Plans are sequences of actions that are taken by an agent to accomplish the goal.</p>
</li>
</ol>
<p>For social robots, the sensor outputs build the belief sets to signify the environment around the robots in terms of different values of different parameters. A particular belief set describes a specific situation in which the robot is located in a specific instant of time. Based on the location, a goal is defined for the robot which can be referred to as desire. A BDI interpreter or engine selects a particular action (intention) from a plan library, which is a collection of intentions, based on the situation.</p>
<h2 id="1-5-Rationale"><a href="#1-5-Rationale" class="headerlink" title="1.5. Rationale"></a><em>1.5. Rationale</em></h2><p>Social robots have become integral and inseparable parts of our life. From the hospitals to convention stores like home, social robots are being extensively used to interact with the human in one way or another. The social robots are now considered as a substitute for human wherever and whenever human is not able to be present for the specific tasks. The robots, therefore, are expected to act more as humans and proactivity can be the feature in such robots that will enable this capability. Many studies have tried to develop proactive behaviors in robots. But when it comes to the commercial indoor social robots powered by ROS, there are no well-defined system architectures and implementations. Given the influence of such robots in a commercial indoor environment, this study tries to explore the possibilities of enhancing proactivity in such commercial indoor social robots. This work defines a clear method to implement different activities of a social robot as proactive behaviors by integrating human reasoning-based BDI framework into the system architecture. The work validates the implementation by developing a vision-based proactive behavior enabled by PROFETA (BDI) framework in an indoor social robot, <em>Waldo</em>, operated by ROS. The specific contribution of the study are listed below:</p>
<ol>
<li><p>A validated modular system architecture, with features such as modularity, flexibility, and rational work distribution, that facilitates different logical blocks to be integrated with robots controlled by ROS for proactive behaviors.</p>
</li>
<li><p>A foundation for the development of human-like behaviors in ROS-controlled robots for daily-life applications.</p>
</li>
</ol>
<p>The rest of the paper is organized as follow: Section 2 discusses the related work, while Section 3 explains the case study set up along with the system overview. Section 4 discusses the results, while Section 5 concludes the paper.</p>
<h2 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2. Related Works"></a>2. Related Works</h2><p>In this section, we describe the background of different BDI models and different frameworks built to integrate behavior models in robots. Furthermore, we present different works where researchers have tried to incorporate such models in robots for various daily life applications.</p>
<h3 id="2-1-BDI-Model"><a href="#2-1-BDI-Model" class="headerlink" title="2.1. BDI Model"></a>2.1. BDI Model</h3><p>Whenever the design of the cognition model for software agents comes into play, the BDI model is one of the most popular architectural choices. BDI models provide an explicit and declarative representation of informational attitudes, motivational attitudes, and deliberative commitments. Myers et al. [16] divided the BDI models into two broad categories of B-DOING and Delegative models. In the B-DOING model, motivational attitudes are highly adapted, and desires correspond to what the agent wishes. Furthermore, obligations corresponded to the responsibilities of other agents and norms correspond to conventions derived from the agent’s role in the environment. The goal created for the agent needs to be consistent and achievable [17]. According to the definition of the goal, the intentions for executions are planned. In the delegative model, the goals are defined as candidate goals and adopted goals [18]. Candidate goals are those that can be inconsistent internally while Adopted goals are the consistent and coherent ones in the BDI model. This model can even incorporate user-specified guidance and preferences from the user in the form of advice. The B-DOING framework lacks the distinctions between types of goals for proactive assistance, while the delegative BDI framework lacks the distinctions between types of motivational attitudes [16].</p>
<h3 id="2-2-BDI-Frameworks"><a href="#2-2-BDI-Frameworks" class="headerlink" title="2.2. BDI Frameworks"></a>2.2. BDI Frameworks</h3><p>Russel et al. [19] developed the agent factory framework as an open-source collection of various tools, platforms and languages that ultimately facilitate the development and development of multi-agent systems. Winikoff [20] built a highly portable, robust and cross-platform environment called JACK for building, running and integrating commercial-grade multi-agent systems. In the BDI framework called JADE [21], the agent platform can be distributed among different independent machines and controlled remotely. The configuration can even be changed at run-time by the moving agents from one machine to another one during the implementation. Braubach and Pokahr [22] developed a framework called <em>JADEX</em>, based on XML and Java, that follows BDI model and facilitates easier intelligent agent system construction with an engineering perspective. JASON is a super flexible platform developed as an extension of AgentSpeak [23] by [24], that implements the semantics of the language and provides a good platform for development of multi-agent systems with many customizable features. The comparison between different behavior model platform is given in Table 1.</p>
<p><strong>Table 1.</strong> Comparison between different beleif–desire–intention (BDI) platforms.</p>
<table>
<thead>
<tr>
<th><strong>Name</strong></th>
<th><strong>Primary  Domain</strong></th>
<th><strong>Open</strong>  <strong>Source</strong></th>
<th><strong>Learn-</strong>  <strong>Ability</strong></th>
<th><strong>Programming  Language</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Agent Factory</td>
<td>General  purpose agent based</td>
<td>Yes</td>
<td>Average</td>
<td>Java,  AgentSpeak</td>
</tr>
<tr>
<td>JACK</td>
<td>Dynamic and Complex environment</td>
<td>No</td>
<td>Easy</td>
<td>Java</td>
</tr>
<tr>
<td>JADE</td>
<td>Distributed applications composed  autonomous entities</td>
<td>Yes</td>
<td>Easy</td>
<td>Java</td>
</tr>
<tr>
<td>JADEX</td>
<td>Distributed applications composed  autonomous BDI entities</td>
<td>Yes</td>
<td>Easy</td>
<td>Java</td>
</tr>
<tr>
<td>BDI4JADE</td>
<td>Enterprise  application</td>
<td>Yes</td>
<td>Average</td>
<td>Java</td>
</tr>
<tr>
<td>JASON</td>
<td>Distributed applications composed  autonomous BDI entities</td>
<td>Yes</td>
<td>Easy</td>
<td>Java</td>
</tr>
<tr>
<td>PROFETA</td>
<td>Distributed  applications composed autonomous BDI entities with AI applications</td>
<td>Yes</td>
<td>Easy</td>
<td>Python</td>
</tr>
<tr>
<td>SPYSE</td>
<td>Distributed  AI applications</td>
<td>Yes</td>
<td>Average</td>
<td>Python</td>
</tr>
<tr>
<td>SPADE</td>
<td>Distributed  Multi-agent</td>
<td>No</td>
<td>Average</td>
<td>Python</td>
</tr>
</tbody></table>
<p>ROS supports the C++ and Python programming languages for communication between different distributed nodes in its ecosystem. Because of various BDI frameworks available in Python, a pythonic framework is considered in this study.</p>
<h3 id="2-3-Application-of-BDI-Models-in-Robots"><a href="#2-3-Application-of-BDI-Models-in-Robots" class="headerlink" title="2.3. Application of BDI Models in Robots"></a>2.3. Application of BDI Models in Robots</h3><p>The behavior model was adapted to study the natural engagement of robots with humans to show exhibit proactive behaviors. The proactive behaviors in robots were imagined to increase the human-robot interaction and utility value in the use of robots. As such, the works related to proactive behavior in robots were initiated with mixed-initiative approaches. Finzi and Orlandini [25] developed an architecture based on a planner mixed-initiative approach for robots used in search and rescue operations. The study had a model-based execution monitoring and reactive planner for the execution of the tasks. Adams et al. [26] proposed an effect based mixed-initiative interaction approach for human-robot interaction. The robot took initiatives upon changes in human emotions like detecting drowsiness and inattentiveness. The robot, as developed by Acosta et al. [27] showed some proactive behaviors by monitoring activities and defining tasks as a schedule. Satake et al. [28] proposed a behavior model to initiate a conversation with pedestrians walking on the streets. The appropriate instant of time to start the conversation or interaction with people was studied in work done by Shi et al. [29]. Moreover, Garrel et al. [30] proposed a behavior model for a proactive model that tries to convince people to initiate a conversation with different behaviors and emotions. The study carried out by Araiza-Illan et al. [31] proposed the use of the BDI model to increase the level of realism and human-like simulation of the robots. An automated testbench was implemented for simulation of cooperative task assembly between a humanoid robot and people in the robot operating system and Gazebo. A soccer-playing robot based on BDI architecture was developed by Gottifredi et al. [32] which allowed the specification of declarative goal-driven behavior based on high-level reasoning and reactivity when required. The work of Duffy et al. [33] developed a multi-layered BDI architecture with an egocentric robot control strategy to make robots capable of explicit social behavior. Pereira et al. [34] proposed an extension to BDI architecture to support artificial emotions in the form of emotional-BDI architecture.</p>
<p>Given the current state of proactivity in social robots, this study tries to extend the capabilities of such robots to include vision-based activity in a social robot. The integration is based on a modular architecture onto which other logical blocks can easily be integrated for more advanced proactive behaviors in a think-like-human fashion.</p>
<h2 id="3-Case-Study-Setup"><a href="#3-Case-Study-Setup" class="headerlink" title="3. Case Study Setup"></a>3. Case Study Setup</h2><p>In this section, we explain the use case scenario created to develop a proactive behavior in a social robot <em>Waldo</em>. Furthermore, we define a system framework based on existing technologies to integrate modular blocks of OpenCV and BDI reasoning with the ROS ecosystem with a detailed explanation of each step as follows.</p>
<h3 id="3-1-Use-Case-Scenario"><a href="#3-1-Use-Case-Scenario" class="headerlink" title="3.1. Use Case Scenario"></a>3.1. Use Case Scenario</h3><p>For this study, we consider a visual image-based activity for adding a proactivity behavior in the social robot. The indoor robot Waldo has cameras installed in its eyes which gather the image feeds from the environment. This image-feeds help to set up a belief about the situation. The camera feeds are processed using a module in OpenCV for person detection. This module establishes a belief for the robot concerning the presence of a person in the environment. In case a person detected by the OpenCV module, Waldo sets up a goal of greeting the person without any explicit commands from the person. In this experiment, the robot can perform two precise actions. Upon the detection of the person, using the BDI framework, the robot greets the detected people with a sentence. Upon continuous detection of the person for a fixed time, the robot changes its belief and offers additional help to the person by speaking a different sentence. For this work, the actions which Waldo can perform is only limited to speech, but advanced services can easily replace these actions. This provision is made in the experiment to show how beliefs can be changed according to the environment so that the actions taken by the robot can be relevant and more interactive.</p>
<h3 id="3-2-Social-Indoor-Robot—Waldo"><a href="#3-2-Social-Indoor-Robot—Waldo" class="headerlink" title="3.2. Social Indoor Robot—Waldo"></a>3.2. Social Indoor Robot—Waldo</h3><p>The robot under consideration for this study is Waldo, which is a multi-service robot manufactured by Immersive Robotics [35]. Waldo is a telepresence service robot with advanced vision capabilities. The robot has an Arduino card for basic control functions and a Linux card with ROS installed as an operating system for more advanced and elaborate functionalities. Waldo has an adjustable height of 130 cm to 170 cm. The robot is autonomous with LIDARs, sonars, microphones, and cameras. Waldo is built in a humanoid shape as an indoor social robot for a welcoming, talking, understanding and communicating with people. The movement of Waldo can be controlled remotely by using a mouse, keyboard, joysticks, pads, smartphones, tablets or any other desired peripherals. Waldo is shown in Figure 1.</p>
<p><strong>Figure 1.</strong> Waldo.</p>
<h3 id="3-3-System-Overview"><a href="#3-3-System-Overview" class="headerlink" title="3.3. System Overview"></a>3.3. System Overview</h3><p>The main goal of this study is to develop a flexible and modular framework using existing technologies that could facilitate the integration of different blocks as modules to the framework,</p>
<p>which ultimately contribute in the development of proactive behaviors in a social robot controlled by ROS. The overall system overview for achieving the behavior model of the environment in indoor models is reflected in Figure 2. Different sensors installed in the indoor robot Waldo collect the information about the environment. The cameras, LiDAR and Kinect sensor help to collect information about the environment for a given instant of time. The information is relayed to Waldo PC, which is connected to the monitoring PC through a wireless connection. Because of the limited capabilities of Waldo PC, the compute-intensive logical modules can be run on a more powerful monitoring PC. The logical models are responsible for deriving various knowledge from the data about the environment collected by sensors of the robot. Based on this knowledge base, the BDI framework establishes the beliefs and set up goals for any given instant of time. The framework also chooses a set of actions from the predefined plan list to accomplish the goal. The actions are relayed to <em>Waldo</em> PC through a wireless connection that directs different actuators in <em>Waldo</em> to perform a wide range of action. The internal mechanisms in ROS manage the communication between the nodes and <em>Waldo</em> PC. The Waldo PC, which is equipped with Linux and ROS, can have several logical modules installed on it or monitoring PC to establish one or more beliefs about the environment. The modules in the logical layer can be BDI framework, OpenCV for vision processing and other intelligent bricks for establishing important beliefs about the environment. The BDI framework responds to the established beliefs and set up goals for any instant of time. The engine in the framework chooses a plan of actions from the predefined libraries for goal accomplishment. The plan execution is relayed to Waldo PC,</p>
<p>which generates actual actions in the robot to respond to the environment. The flow operation can be monitored using a monitoring PC connected to Waldo PC over a wireless network.</p>
<p><strong>Figure 2.</strong> System overview.</p>
<h3 id="3-4-BDI-Modeling"><a href="#3-4-BDI-Modeling" class="headerlink" title="3.4. BDI Modeling"></a>3.4. BDI Modeling</h3><p>The BDI modeling of the problem should be able to answer the following question effectively and efficiently.</p>
<p>•      When to act? The robot works whenever it detects a person inside the room. So, an efficient mechanism for the detection of a person should be integrated into the robot. An effective block to trigger actions has to be adopted.</p>
<p>•      What actions to take? The actions which are expected to be made by robots hugely depend upon the detection of the people. The action can be a simple greeting message delivered to the person or message delivered to the person offering some help or no action at all.</p>
<p>•      How to perform the actions? Based on the set of beliefs about the environment, the robot can decide to take action. For any case of detection of a person, the robot employs its text to speech node for speaking out sentences to either greet or offer help to the people. For non-detection of the person, the robot can deliberate itself to stay idle or go to sleep mode.</p>
<p>The knowledge about the surrounding is collected by the cameras installed on the eyes of the robot. For the experiment, the camera feeds are only used. The person detection block from OpenCV is used to define the belief for the system design explicitly. Moreover, the goals and actions are defined accordingly to realize a use case of proactive behavior. The possible actions which the robot could take were limited only to the speech. The set of beliefs, desires, and intentions as per BDI model for the case</p>
<p>study are defined as follows:</p>
<p><strong>Belief</strong>: personDetected(“Yes”), personDetected(“No”) and personDetected(“Next”)</p>
<p><strong>Desire</strong>: DoNothing(), GreetPeople() and OfferHelp()</p>
<p><strong>Intentions</strong>: stayIdle(), speak()</p>
<p>The PROFETA framework can be implemented for behavior modeling using the following steps:</p>
<p>​       </p>
<p><strong>Algorithm 1</strong> Implementation of PROFETA framework.</p>
<p>​       </p>
<p>1: Import necessary PROFETA libraries</p>
<p>2: Define beliefs and goals as classes in the script</p>
<p>3: Define user actions by creating classes and overriding the method execute()</p>
<p>4: Start PROFETA engine</p>
<p>5: Define rules by using declarative syntax</p>
<p>6: Run the engine</p>
<p>​       </p>
<p>Moreover, PROFETA framework also facilitates the definition of sensor class which can itself add or remove a set of beliefs depending upon the environment. This can be done in PROFETA by declaring a subclass Sensor, overriding the sense() method and informing the PROFETA engine about the addition of a new sensor in the program. PROFETA uses declarative language so as to express the behaviors of agents. The declarative syntax for the behavior of an agent is described below: “Event”/“Condition” » “setofActions”</p>
<p>In this declarative syntax, an event can be any one of belief assert or retract, goal accomplishment or request or even goal failures. The condition in the syntax refers to a particular set of knowledge base while actions can be goal accomplishment request, user-defined set of actions, or adding or removing</p>
<p>beliefs. This syntax can be exemplified as:</p>
<p>+objectAt(“A”, “B”)/objectGot(“no”) » [moveTo(“A”,“B”), pickObject()]</p>
<h3 id="3-5-Person-Detection-Using-OpenCV"><a href="#3-5-Person-Detection-Using-OpenCV" class="headerlink" title="3.5. Person Detection Using OpenCV"></a>3.5. Person Detection Using OpenCV</h3><p>For person detection, open-source computer vision library (OpenCV), a freely available open-source library for computer vision and machine learning, is used. The libraries and algorithms in OpenCV are directly used in the experiments to detect a person based on the histogram of oriented gradient (HOG) features and support vector machine (SVM) classifier. The performance improvement of the OpenCV algorithms for person detection is beyond the scope of this work. ROS has its own image format used for communication between nodes through subscription and publishing. This image format has to be converted into OpenCV format to use the OpenCV libraries for person detection. CvBridge, a library in ROS, facilitates the conversion of ROS images to OpenCV image format and vice-versa. The CvBridge interface is represented in Figure 3.</p>
<p><strong>Figure 3.</strong> CvBridge interface.</p>
<h3 id="3-6-Experimental-Setup"><a href="#3-6-Experimental-Setup" class="headerlink" title="3.6. Experimental Setup"></a>3.6. Experimental Setup</h3><p>The test use case is implemented in Waldo with the works distributed over two PCS. The workstation PC has Ubuntu 16.04 installed on an Intel i5-5300U processor with ROS Kinetic whereas the Waldo PC has the same operating system installed on an Intel(R) Atom(TM) processor.</p>
<p>The proactive behavior of <em>Waldo</em> is well implemented using the BDI framework of PROFETA.</p>
<h2 id="4-Results-and-Discussion"><a href="#4-Results-and-Discussion" class="headerlink" title="4. Results and Discussion"></a>4. Results and Discussion</h2><p>In this section, we present the validation of the proposed system design for the integration of various logical modules like the BDI model and OpenCV into ROS for proactive behaviors. The capabilities of the proposed framework are validated using a qualitative approach, where several features such as modularity, flexibility, and rational work distribution are investigated. Furthermore,</p>
<p>we explain the associated findings in detail and include a quantitative analysis of the results for logical OpenCV module.</p>
<h3 id="4-1-Work-Distribution-in-Proposed-System-Architecture"><a href="#4-1-Work-Distribution-in-Proposed-System-Architecture" class="headerlink" title="4.1. Work Distribution in Proposed System Architecture"></a>4.1. Work Distribution in Proposed System Architecture</h3><p>One of the key features expected from the proposed framework is the rational distribution of works required in the development of proactive behaviors in the robot. The study implements the test use case in ROS with an overlaid layer of the BDI framework of PROFETA. A module in the OpenCV library does the image processing. The proposed system design distributed the operations over two PCs, Waldo PC, installed in the robot and Workstation PC. The test experiments utilized the distributed working architecture of ROS as the design offloaded the Waldo PC from heavy processing of image feeds collected from the cameras. The bulky and more compute-intensive modules of person detection and practical reasoning (BDI framework) are installed in a comparatively powerful Workstation PC. The actual actions and the management of different ROS nodes are handled in Waldo PC. These actions are not compute-intensive. As such, the proposed architecture supports a reasonable work distribution in the development of proactive behaviors. Consequently, more advanced activities can be thought of as an extension as more compute-intensive modular blocks can be easily integrated into the robot, thanks to the architecture. More powerful machines can assume the role of Workstation PC for such capabilities while Waldo PC can assume the light roles of information collection and effectors. Moreover, the framework allows us to add additional computational devices to consider different workloads required for various proactive behaviors. Such capabilities make the proposed framework flexible as well.</p>
<h3 id="4-2-Validation-of-Proposed-System-Design-with-Test-Use-Case"><a href="#4-2-Validation-of-Proposed-System-Design-with-Test-Use-Case" class="headerlink" title="4.2. Validation of Proposed System Design with Test Use Case"></a>4.2. Validation of Proposed System Design with Test Use Case</h3><p>The validation of the proposed system design focused on investigating whether the features of flexibility, modularity, and rational work distribution are achieved during the exhibition of proactive behavior by Waldo. For validating the proposed system design, we created several distributed nodes within and outside the ROS ecosystem. There is one ROS node each for camera feed, person detection block and speech block. The different ROS nodes communicate with each other through the topic messages. Roscore manages the communication between the nodes. As required, the nodes can be created and removed for adding or removing functionalities. The nodes can be created in any of the computing devices available within the proposed framework, providing flexibility and modularity. Initially, the camera node publishes the image feed collected by the eyes of Waldo. The converter node, with CvBridge interface, subscribes to the topic messages of camera node and converts the ROS images to OpenCV images. The node then publishes the converted images. There is a node called person detector running in Workstation PC, that subscribes to the converted image messages. This node executes the person detection module of OpenCV library. There is an additional node called BDI engine in Workstation PC that subscribes to the messages about person detection published by the person detector. BDI engine executes all the necessary behavior modeling to publish the action to be done by the robot finally. There is another node, listener, that subscribes to BDI engine node and publishes messages for actuators in the robot to undertake the actions. The speech node that subscribes to listener makes the robot speak out the sentences to achieve the goal. The entire interaction between different nodes in a widely distributed ROS ecosystem is shown in Figure 4. As can be seen in the figure, different workloads required for the proactive behaviors are distributed over Waldo PC and Workstation PC. Such rational work distribution is one of the strengths of the proposed framework, where computing devices can be easily added or removed based on the requirements. Furthermore, additional logical blocks/modular can be created as new nodes within the ROS ecosystem in the proposed framework to develop additional functionality in the social robot. In our study, we created a node for the movement of the head of the robot during the person detection to demonstrate the modularity in the proposed framework. The newly created node communicates with the person detector node to create any movement.</p>
<p><strong>Figure 4.</strong> Interaction between different nodes in robot operating system (ROS) ecosystem.</p>
<h3 id="4-3-Performance-Analysis"><a href="#4-3-Performance-Analysis" class="headerlink" title="4.3. Performance Analysis"></a>4.3. Performance Analysis</h3><p>For the performance analysis and evaluation, Waldo is kept at a fixed position in the corridor. The eyes of the robot, which have cameras, are at the height of about 162 cm. The tests are carried repeatedly where both natural and artificial lights influence the lighting condition.</p>
<p>Figures 5 and 6 represent the execution of BDI actions and image feeds collected by the camera installed in the robot, for no person detected and person detected cases, respectively. When a person is not detected in the image feed, the BDI engine establishes the belief about the environment and create a goal of not greeting the people. Accordingly, the engine selects the action of NoTalk to achieve the goal, as shown in Figure 5a. Similarly, when a person is detected, the BDI engine asserts a belief of encountering a person, enabled by the person detection module running on the Workstation PC. Based on this belief set, the engine has to set up a goal of either greeting the people or offering additional help to the people. For the distinction between the two, we add logical operation of tracking for how long the person has been encountered, as represented by the counter in Figure 6a. Based on the value of counter and person detected in the image feed, the BDI engine establishes two distinct belief sets requiring two different goals. The greeting action to achieve the goal of greeting people is represented as talk action in Figure 6a. The help action is executed to achieve the purpose of offering help to people when the person is detected continuously for a more extended period.</p>
<p>​        </p>
<p>​                               (<strong>a</strong>)                                 (<strong>b</strong>)</p>
<p><strong>Figure 5.</strong> Case: no person detected (<strong>a</strong>) BDI execution and (<strong>b</strong>) image feed collected by eyes of Waldo.</p>
<p>​        </p>
<p>​                               (<strong>a</strong>)                                 (<strong>b</strong>)</p>
<p><strong>Figure 6.</strong> Case: person detected (<strong>a</strong>) BDI execution and (<strong>b</strong>) image feed collected by eyes of Waldo.</p>
<p>Moreover, we tested the working of the overall system considering different distances from the robot in two different scenes. The analysis of the performance is presented in Tables 2 and 3.</p>
<p><strong>Table 2.</strong> Distance consideration for the validation of the work.</p>
<table>
<thead>
<tr>
<th><strong>Distance</strong></th>
<th></th>
<th><strong>Scene  1</strong></th>
<th></th>
<th></th>
<th><strong>Scene  2</strong></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><strong>(of  Person from Camera)</strong></td>
<td><strong>True Positives</strong></td>
<td><strong>False Positives</strong></td>
<td><strong>False Negatives</strong></td>
<td><strong>True Positives</strong></td>
<td><strong>False Positives</strong></td>
<td><strong>False Negatives</strong></td>
</tr>
<tr>
<td>&lt;2 m</td>
<td>253</td>
<td>160</td>
<td>187</td>
<td>175</td>
<td>160</td>
<td>265</td>
</tr>
<tr>
<td>2–10 m</td>
<td>401</td>
<td>87</td>
<td>112</td>
<td>354</td>
<td>114</td>
<td>132</td>
</tr>
</tbody></table>
<p><strong>Table 3.</strong> Precision and recall evaluation.</p>
<table>
<thead>
<tr>
<th><strong>Distance</strong></th>
<th></th>
<th><strong>Scene 1</strong></th>
<th></th>
<th></th>
<th><strong>Scene 2</strong></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><strong>(of  Person from Camera)</strong></td>
<td><strong>Precision</strong></td>
<td><strong>Recall</strong></td>
<td><strong>F1 Score</strong></td>
<td><strong>Precision</strong></td>
<td><strong>Recall</strong></td>
<td><strong>F1 Score</strong></td>
</tr>
<tr>
<td>&lt;2 m</td>
<td>0.612</td>
<td>0.575</td>
<td>0.593</td>
<td>0.52</td>
<td>0.398</td>
<td>0.451</td>
</tr>
<tr>
<td>2–10 m</td>
<td>0.822</td>
<td>0.782</td>
<td>0.801</td>
<td>0.756</td>
<td>0.728</td>
<td>0.742</td>
</tr>
</tbody></table>
<p>The performance analysis of the entire system showed that the robot performed quite poorly for the region beyond 2–10 m. The respective values of precision and recall for Scene 1 are 0.822 and 0.782, while the same for Scene 2 are 0.756 and 0.728. Scene 2 had more lighting inconsistencies when compared to Scene 1. The false negatives and positives (as shown in Figures 7 and 8) during the test</p>
<p>were caused by the change in lighting conditions brought about by the movement of objects. Moreover, such negatives were caused by the shadows of the people formed by several sources of lights (natural and artificial). Because of the unequal distribution of data sets, we also calculated the F1-Score for each scenario. The best F1-score was obtained for Scene 1 as 0.801 for the region of 2–10 m. The high value of F1-score (closer to 1) shows the efficiency of the module when detecting the person so that the robot can exhibit proactive behavior in greeting and offering help to the person detected. Similar to the analysis given by precision and recall, F1-score for Scene 2 in the region of &lt;2 m is the lowest, highlighting the fact that the logical block does not perform so effectively in that location. Furthermore,</p>
<p>we plotted the precision-recall curve for our experiments, which are shown in Figure 9. The curve also confirms the findings and establishes the best performance of the module in Scene 1 (2–10 m region) as the area under the curve for the case is the highest (see Figure 9). The performance of the module is affected by several factors such as artificial light, shadow, multiple sources of light and the distance between the camera and the person. The accuracy of the system design in the given test case can be improved by enhancing the performance of the OpenCV logical block.</p>
<p><strong>Figure 7.</strong> False negatives.</p>
<p><strong>Figure 8.</strong> False positives.</p>
<p><strong>Figure 9.</strong> Precision-recall curve.</p>
<h3 id="4-4-Limitation"><a href="#4-4-Limitation" class="headerlink" title="4.4. Limitation"></a>4.4. Limitation</h3><p>For this study, the image frames without the entire body of the person are not considered. Since the cameras are placed at a certain height and the angular movement of the head is limited, the camera cannot cover the distance that was less than a meter. Thus, the region in front of the robot, less than 2 m away, is a blind spot. Such blind spots give poor performance in relation to person detection. Moreover, beyond the distance of about 10 m, the person detection module is not able to detect the person. As for the system design, the architecture still offers modularity and flexibility. The accuracy of the entire system is largely dependent upon the logical block used to develop proactive behaviors in the robot. Apart from the limitation in the logical block, the study focuses more on the qualitative validation of the proposed framework, where the features, such as modularity, flexibility, and rational work distributions, are investigated. The work is primarily determined in developing a basic flexible foundation using existing and freely available tools for the development of proactive behaviors in social robots. Because of no readily available comparable architecture and case studies, a comparative analysis is not included in the paper. Moreover, the advanced capabilities of the social robot are also not considered as of now. These limitations of the study will be enhanced in the future.</p>
<h2 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5. Conclusions"></a>5. Conclusions</h2><p>The recent rise of humanoid and animaloid indoor social robots in commercial spaces, including home has necessitated the “act-like-human” behavior in those robots for a more friendly human-robot interaction. The proactivity in such social robots adds more utility to the robots. As such, in this paper, we presented a validated use-case of such proactive behavior in an indoor commercial social robot, <em>Waldo</em> enabled by the behavior model framework, PROFETA. We clearly defined a fundamental system architecture with the features of flexibility, modularity, and rational work distribution to integrate the BDI framework into the distributed ecosystem of ROS. In the architecture, multiple ROS nodes can be independently created over multiple machines, connected by wireless communication.</p>
<p>We demonstrated how an external module, such as OpenCV library, can be used to enhance the capability of an indoor robot in a plug-and-play fashion. We expect the proposed system architecture to lay a solid foundation to develop a wide range of proactive behaviors in an indoor social robot to behave and act like a human. Such behaviors can be possible with the addition of various logical modules into the proposed architecture.</p>
<p>This work validated the working of the proposed architecture with the basic actions of the robot. Preliminary works are underway to integrate the blocks of artificial intelligence into the proposed architecture to develop more intelligent actions in the robot. The expansion of the actions which Waldo can perform, is also ongoing. The future work can be centered around establishing a more accurate belief within the human reasoning paradigm, BDI, by using data collected by multiple sensors in the robot. Other than the camera, LiDAR and Kinect sensor can be used to represent the state of the environment better. In terms of the action of the robot, further works can focus on integrating the autonomous navigation of the robot to achieve the goal set up based on different beliefs. The addition of learning mechanisms, to keep improving, can be thought of as an essential extension.</p>
<p><strong>Author Contributions:</strong> U.K.C. contributed to the project conceptualization, literature review, methodology, tool integration, testing, validation and experimentation. J.C. contributed to the project development, administration, conceptualization, methodology, validation and supervision. Both authors contributed to the paper writing, reviewing, and editing.</p>
<p><strong>Funding:</strong> This research received no external funding.</p>
<p><strong>Acknowledgments:</strong> The authors would like to thank all the members of Home and Family Communication (HFC) team of Orange Labs, Lannion for their kind help and support during the project. Additionally, the authors are grateful to everyone who helped us to improve the paper quality during various stages of paper publication.</p>
<p><strong>Conflicts of Interest:</strong> The authors declare no conflict of interest.</p>
<h1 id="Appendix-A"><a href="#Appendix-A" class="headerlink" title="Appendix A"></a>Appendix A</h1><p><strong>Table A1.</strong> Reactive vs. Proactive Behaviors.</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>Reactive</strong></th>
<th><strong>Proactive</strong></th>
</tr>
</thead>
<tbody><tr>
<td>User</td>
<td>Could you  help me?</td>
<td><strong>It seems like your computer</strong></td>
</tr>
<tr>
<td>Robot</td>
<td>Yes, What  is wrong?</td>
<td><strong>is  not responding, let me know if I can do anything?</strong></td>
</tr>
<tr>
<td>User</td>
<td>It seems like my computer is not  responding</td>
<td>Could you  help me?</td>
</tr>
<tr>
<td>Robot</td>
<td>You should  end the process</td>
<td>You should  end the process</td>
</tr>
<tr>
<td></td>
<td>by  pressing ALT+CTRL+DEL.</td>
<td>by  pressing ALT+CTRL+DEL.</td>
</tr>
<tr>
<td>User</td>
<td>Won’t that  restart my computer?</td>
<td>Won’t that  restart my computer?</td>
</tr>
<tr>
<td>Robot</td>
<td>Don’t  worry; it will just</td>
<td>Don’t  worry; it will just</td>
</tr>
<tr>
<td></td>
<td>end the  process.</td>
<td>end the  process.</td>
</tr>
<tr>
<td>User</td>
<td>Hmm, it’s  not doing anything.</td>
<td>OK</td>
</tr>
<tr>
<td>Robot</td>
<td>Press  ALT+CTRL+DEL together.</td>
<td>Press  ALT+CTRL+DEL together.</td>
</tr>
<tr>
<td>User</td>
<td>Ok and  now?</td>
<td>Then you  should end the process.</td>
</tr>
<tr>
<td>Robot</td>
<td>Then you  should end the process.</td>
<td><strong>Do you have a back-up?</strong></td>
</tr>
<tr>
<td>User</td>
<td>But then I  lost my work?</td>
<td>I’m not  sure.</td>
</tr>
<tr>
<td>Robot</td>
<td>Don’t  worry your computer makes a</td>
<td>Don’t  worry your computer makes a</td>
</tr>
<tr>
<td></td>
<td>backup  every five minutes</td>
<td>backup  every five minutes</td>
</tr>
<tr>
<td>User</td>
<td>How can I  find a backup?</td>
<td>Where is  my backup?</td>
</tr>
<tr>
<td>Robot</td>
<td>You can  open your program</td>
<td>To find  your program, you can</td>
</tr>
<tr>
<td></td>
<td>again and  select the file.</td>
<td>open your program again and  select the file. Try it!</td>
</tr>
</tbody></table>
<p>Note: The bold texts are the proactive behaviors (adopted from [13]).</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><p>Fong, T.; Nourbakhsh, I.; Dautenhahn, K. A survey of socially interactive robots. <em>Robot. Auton. Syst.</em> <strong>2003</strong>, <em>42</em>, 143–166.</p>
</li>
<li><p>Campa, R. The rise of social robots: a review of the recent literature. <em>J. Evol. Technol.</em> <strong>2016</strong>, <em>26</em>, 106–113.</p>
</li>
<li><p>Dautenhahn, K. Socially intelligent robots: dimensions of human–robot interaction. <em>Philos. Trans. R. Soc. B Biol. Sci.</em> <strong>2007</strong>, <em>362</em>, 679–704.</p>
</li>
<li><p>Breazeal, C.L. <em>Designing Sociable Robots</em>; MIT Press: Cambridge, MA, USA, 2004.</p>
</li>
<li><p>Dautenhahn, K. The art of designing socially intelligent agents: Science, fiction, and the human in the loop.</p>
</li>
</ol>
<p><em>Appl. Artif. Intell.</em> <strong>1998</strong>, <em>12</em>, 573–617.</p>
<ol start="6">
<li>Ferland, F.; Agrigoroaie, R.; Tapus, A. Assistive Humanoid Robots for the Elderly with Mild Cognitive</li>
</ol>
<p>Impairment. In <em>Humanoid Robotics: A Reference</em>; Springer: Berlin/Heidelberg, Germany, 2019; pp. 2377–2396.</p>
<ol start="7">
<li><p>Flandorfer, P. Population ageing and socially assistive robots for elderly persons: The importance of sociodemographic factors for user acceptance. <em>Int. J. Popul. Res.</em> <strong>2012</strong>, <em>2012</em>, 829835.</p>
</li>
<li><p>Meghdari, A.; Shariati, A.; Alemi, M.; Vossoughi, G.R.; Eydi, A.; Ahmadi, E.; Mozafari, B.; Amoozandeh Nobaveh, A.; Tahami, R. Arash: A social robot buddy to support children with cancer in a hospital environment. <em>Proc. Inst. Mech. Eng. Part H J. Eng. Med.</em> <strong>2018</strong>, <em>232</em>, 605–618.</p>
</li>
<li><p>Conti, D.; Di Nuovo, S.; Buono, S.; Di Nuovo, A. Robots in education and care of children with developmental disabilities: a study on acceptance by experienced and future professionals. <em>Int. J. Soc. Robot.</em> <strong>2017</strong>, <em>9</em>, 51–62. </p>
</li>
<li><p>Belpaeme, T.; Kennedy, J.; Ramachandran, A.; Scassellati, B.; Tanaka, F. Social robots for education: A review.</p>
</li>
</ol>
<p><em>Sci. Robot.</em> <strong>2018</strong>, <em>3</em>, eaat5954.</p>
<ol start="11">
<li><p>Siciliano, B.; Khatib, O. <em>Springer Handbook of Robotics</em>; Springer: Berlin/Heidelberg, Germany, 2016.</p>
</li>
<li><p>Bartneck, C.; Forlizzi, J. A design-centred framework for social human-robot interaction. In Proceedings of the RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No. 04TH8759), Kurashiki, Japan, 22–22 September 2004; pp. 591–594.</p>
</li>
<li><p>Kemper, N. Effects of Proactive Behavior and Physical Interaction with a Social Robot. Master’s Thesis, University of Amsterdam, Amsterdam, The Netherlands, 2009.</p>
</li>
<li><p>Quigley, M.; Conley, K.; Gerkey, B.; Faust, J.; Foote, T.; Leibs, J.; Wheeler, R.; Ng, A.Y. ROS: An open-source Robot Operating System. In Proceedings of the ICRA Workshop on Open Source Software, Kobe, Japan, 17 May 2009; Volume 3, p. 5.</p>
</li>
<li><p>Bratman, M. <em>Intention, Plans, and Practical Reason</em>; Harvard University Press Cambridge: Cambridge, MA, USA, 1987; Volume 10.</p>
</li>
<li><p>Myers, K.; Yorke-Smith, N. Proactive behavior of a personal assistive agent. In Proceedings of the AAMAS Workshop on Metareasoning in Agent-Based Systems, Honolulu, HI, USA, 14 May 2007; pp. 31–45.</p>
</li>
<li><p>Dignum, F.; Kinny, D.; Sonenberg, L. From desires, obligations and norms to goals. <em>Cogn. Sci. Q.</em> <strong>2002</strong>, <em>2</em>, 407–430.</p>
</li>
<li><p>Myers, K.L.; Yorke-Smith, N. A cognitive framework for delegation to an assistive user agent. In Proceedings of the AAAI 2005 Fall Symposium on Mixed-Initiative Problem-Solving Assistants, Arlington, Virginia, 4–6 November 2005; pp. 94–99.</p>
</li>
<li><p>Russell, S.; Jordan, H.; O’Hare, G.M.; Collier, R.W. Agent factory: A framework for prototyping logic-based AOP languages. In Proceedings of the German Conference on Multiagent System Technologies, Berlin, Germany, 6–7 October 2011; pp. 125–136.</p>
</li>
<li><p>Winikoff, M. JACKTM intelligent agents: An industrial strength platform. In <em>Multi-Agent Programming</em>; Springer: Berlin/Heidelberg, Germany, 2005; pp. 175–193.</p>
</li>
<li><p>Jedrzejowicz, P.; Wierzbowska, I. JADE-Based a-team environment. In Proceedings of the International Conference on Computational Science, Reading, UK, 28–31 May 2006; pp. 719–726.</p>
</li>
<li><p>Braubach, L.; Pokahr, A. The jadex project: Simulation. In <em>Multiagent Systems and Applications</em>; Springer: Berlin/Heidelberg, Germany, 2013; pp. 107–128.</p>
</li>
<li><p>Rao, A.S. AgentSpeak (L): BDI agents speak out in a logical computable language. In Proceedings of the European Workshop on Modelling Autonomous Agents in a Multi-Agent World, Eindhoven, The Netherlands, 22–25 January 1996; pp. 42–55.</p>
</li>
<li><p>Bordini, R.H.; Hübner, J.F.; Wooldridge, M. <em>Programming Multi-Agent Systems in AgentSpeak Using Jason</em>; John Wiley &amp; Sons: Hoboken, NJ, USA, 2007; Volume 8.</p>
</li>
<li><p>Finzi, A.; Orlandini, A. Human-Robot Interaction Through Mixed-Initiative Planning for Rescue and Search Rovers. In <em>AI*IA 2005: Advances in Artificial Intelligence</em>; Bandini, S., Manzoni, S., Eds.; Springer: Berlin/Heidelberg, Germany, 2005; pp. 483–494.</p>
</li>
<li><p>Adams, J.A.; Rani, P.; Sarkar, N. Mixed initiative interaction and robotic systems. In Proceedings of the AAAI Workshop on Supervisory Control of Learning and Adaptive Systems, San Jose, CA, USA, 25–26 July 2004; pp. 6–13.</p>
</li>
<li><p>Acosta, M.; Kang, D.; Choi, H.J. Robot with emotion for triggering mixed-initiative interaction planning. In Proceedings of the 2008 IEEE 8th International Conference on Computer and Information Technology Workshops, Sydney, Australia, 8–11 July 2008; pp. 98–103.</p>
</li>
<li><p>Satake, S.; Kanda, T.; Glas, D.F.; Imai, M.; Ishiguro, H.; Hagita, N. How to approach humans?: Strategies for social robots to initiate interaction. In Proceedings of the 4th ACM/IEEE International Conference on Human Robot Interaction, La Jolla, CA, USA, 9–13 March 2009; pp. 109–116.</p>
</li>
<li><p>Shi, C.; Shimada, M.; Kanda, T.; Ishiguro, H.; Hagita, N. Spatial Formation Model for Initiating Conversation. In Proceedings of the 7th Annual Robotics: Science and Systems Conference, Los Angeles, CA, USA, 16–20 October 2011; doi:10.15607/RSS.2011.VII.039.</p>
</li>
<li><p>Garrell, A.; Villamizar, M.; Moreno-Noguer, F.; Sanfeliu, A. Proactive behavior of an autonomous mobile robot for human-assisted learning. In Proceedings of the 2013 IEEE RO-MAN, Gyeongju, Korea, 26–29 August 2013; pp. 107–113.</p>
</li>
<li><p>Araiza-Illan, D.; Pipe, T.; Eder, K. Model-Based Testing, Using Belief-Desire-Intentions Agents, of Control Code for Robots in Collaborative Human-Robot Interactions. <em>arXiv</em> <strong>2016</strong>, arXiv:1603.00656.</p>
</li>
<li><p>Gottifredi, S.; Tucat, M.; Corbatta, D.; García, A.J.; Simari, G.R. A BDI architecture for high level robot deliberation. In Proceedings of the XIV Congreso Argentino de Ciencias de la Computación, Chilecito, Argentina, 8–12 August 2008.</p>
</li>
<li><p>Duffy, B.R.; Dragone, M.; O’Hare, G.M. Social robot architecture: A framework for explicit social interaction. In Proceedings of the Toward Social Mechanisms of Android Science: A CogSci 2005 Workshop, Stresa, Italy, 25–26 July 2005.</p>
</li>
<li><p>Pereira, D.; Oliveira, E.; Moreira, N.; Sarmento, L. Towards an architecture for emotional BDI agents. In Proceedings of the 2005 Portuguese Conference on Artificial Intelligence, Covilha, Portugal, 5–8 December 2005; pp. 40–46.</p>
</li>
<li><p>Immersive Robotics. Available online: <a href="http://immersive-robotics.com/" target="_blank" rel="noopener">http://immersive-robotics.com/</a> (accessed on 12 February 2017).</p>
</li>
</ol>
<p>​     2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license <a href="http://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener">(http://creativecommons.org/licenses/by/4.0/)</a>.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/paper/" rel="tag"># paper</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2019/12/22/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%80%83%E8%AF%95%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E5%A4%A7%E5%85%A8/" rel="next" title="研究生考试数学公式大全">
                  <i class="fa fa-chevron-left"></i> 研究生考试数学公式大全
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2019/12/26/Redis%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0/" rel="prev" title="Redis数据库学习">
                  Redis数据库学习 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-介绍"><span class="nav-number">1.</span> <span class="nav-text">1.介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-1-社交机器人"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 社交机器人</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-2-主动性"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 主动性</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-3-机器人操纵系统-ROS"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 机器人操纵系统(ROS)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-4-信念-愿望-意图模型-BDI"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 信念-愿望-意图模型(BDI)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-5-基本原理"><span class="nav-number">1.5.</span> <span class="nav-text">1.5 基本原理</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-相关工作"><span class="nav-number">2.</span> <span class="nav-text">2. 相关工作</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-1-BDI模型"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 BDI模型</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-BDI框架"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 BDI框架</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-3-BDI模型在机器人中的应用"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 BDI模型在机器人中的应用</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-案例研究设置"><span class="nav-number">3.</span> <span class="nav-text">3. 案例研究设置</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#3-1-用例场景"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 用例场景</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-2-社交室内机器人Waldo"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 社交室内机器人Waldo</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-3-系统概述"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 系统概述</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-4-BDI建模"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 BDI建模</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-5-使用OpenCV进行人身检测"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 使用OpenCV进行人身检测</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-6-实验装置"><span class="nav-number">3.6.</span> <span class="nav-text">3.6 实验装置</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-结果与讨论"><span class="nav-number">4.</span> <span class="nav-text">4 结果与讨论</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#4-1-系统架构中的工作分配"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 系统架构中的工作分配</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#4-2-用测试用例验证提出的系统设计"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 用测试用例验证提出的系统设计</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#4-4-限制"><span class="nav-number">4.3.</span> <span class="nav-text">4.4 限制</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-结论"><span class="nav-number">5.</span> <span class="nav-text">5 结论</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#参考文献"><span class="nav-number">5.1.</span> <span class="nav-text">参考文献</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-Proactivity"><span class="nav-number"></span> <span class="nav-text">1.2. Proactivity</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-Robot-Operating-System-ROS"><span class="nav-number"></span> <span class="nav-text">1.3. Robot Operating System (ROS)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-BDI"><span class="nav-number"></span> <span class="nav-text">1.4. BDI</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-Rationale"><span class="nav-number"></span> <span class="nav-text">1.5. Rationale</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Related-Works"><span class="nav-number"></span> <span class="nav-text">2. Related Works</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-BDI-Model"><span class="nav-number"></span> <span class="nav-text">2.1. BDI Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-BDI-Frameworks"><span class="nav-number"></span> <span class="nav-text">2.2. BDI Frameworks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Application-of-BDI-Models-in-Robots"><span class="nav-number"></span> <span class="nav-text">2.3. Application of BDI Models in Robots</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Case-Study-Setup"><span class="nav-number"></span> <span class="nav-text">3. Case Study Setup</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Use-Case-Scenario"><span class="nav-number"></span> <span class="nav-text">3.1. Use Case Scenario</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Social-Indoor-Robot—Waldo"><span class="nav-number"></span> <span class="nav-text">3.2. Social Indoor Robot—Waldo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-System-Overview"><span class="nav-number"></span> <span class="nav-text">3.3. System Overview</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-BDI-Modeling"><span class="nav-number"></span> <span class="nav-text">3.4. BDI Modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Person-Detection-Using-OpenCV"><span class="nav-number"></span> <span class="nav-text">3.5. Person Detection Using OpenCV</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-Experimental-Setup"><span class="nav-number"></span> <span class="nav-text">3.6. Experimental Setup</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Results-and-Discussion"><span class="nav-number"></span> <span class="nav-text">4. Results and Discussion</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Work-Distribution-in-Proposed-System-Architecture"><span class="nav-number"></span> <span class="nav-text">4.1. Work Distribution in Proposed System Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Validation-of-Proposed-System-Design-with-Test-Use-Case"><span class="nav-number"></span> <span class="nav-text">4.2. Validation of Proposed System Design with Test Use Case</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Performance-Analysis"><span class="nav-number"></span> <span class="nav-text">4.3. Performance Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-Limitation"><span class="nav-number"></span> <span class="nav-text">4.4. Limitation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Conclusions"><span class="nav-number"></span> <span class="nav-text">5. Conclusions</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Appendix-A"><span class="nav-number"></span> <span class="nav-text">Appendix A</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number"></span> <span class="nav-text">References</span></a></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Kuang"
      src="/images/kenan.jpg">
  <p class="site-author-name" itemprop="name">Kuang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Isdear" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Isdear" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/kuang8421@qq.com" title="E-Mail → kuang8421@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/269017831" title="B站 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;269017831" rel="noopener" target="_blank"><i class="fa fa-fw fa-youtube-play"></i>B站</a>
      </span>
      <span class="links-of-author-item">
        <a href="/1906956615" title="QQ → 1906956615"><i class="fa fa-fw fa-qq"></i>QQ</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://isdear.github.io/" title="http:&#x2F;&#x2F;isdear.github.io">Title</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kuang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.5.0
  </div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  
















  

  

</body>
</html>
